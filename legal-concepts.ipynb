{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Legal concept-exampl system"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/apohllo/jurix-2023/blob/main/legal-concepts.ipynb)\n",
    "\n",
    "If running in Colab:\n",
    "1. Copy `requirements.txt` to the main dir.\n",
    "2. Create `data/` dir.\n",
    "3. Copy `questions.json` do `data/`.\n",
    "4. Creae `data/gdprhub/` dir.\n",
    "\n",
    "Please not, that running time of sentence search on V100 takes approx. 1h."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 1 - processing of decisions from GDPRHub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download decisions from GDPRHub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests==2.31.0 in /home/apohllo/miniconda3/envs/exaile/lib/python3.11/site-packages (from -r requirements.txt (line 1)) (2.31.0)\n",
      "Requirement already satisfied: tqdm==4.65.0 in /home/apohllo/miniconda3/envs/exaile/lib/python3.11/site-packages (from -r requirements.txt (line 2)) (4.65.0)\n",
      "Requirement already satisfied: stanza==1.5.0 in /home/apohllo/miniconda3/envs/exaile/lib/python3.11/site-packages (from -r requirements.txt (line 3)) (1.5.0)\n",
      "Requirement already satisfied: nltk==3.8.1 in /home/apohllo/miniconda3/envs/exaile/lib/python3.11/site-packages (from -r requirements.txt (line 4)) (3.8.1)\n",
      "Collecting inflect==7.0.0 (from -r requirements.txt (line 5))\n",
      "  Downloading inflect-7.0.0-py3-none-any.whl (34 kB)\n",
      "Collecting matplotlib==3.7.2 (from -r requirements.txt (line 6))\n",
      "  Downloading matplotlib-3.7.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.6/11.6 MB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting pandas==2.0.3 (from -r requirements.txt (line 7))\n",
      "  Downloading pandas-2.0.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.2/12.2 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting scikit-learn==1.3.0 (from -r requirements.txt (line 8))\n",
      "  Downloading scikit_learn-1.3.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.9/10.9 MB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting transformers==4.30.2 (from -r requirements.txt (line 9))\n",
      "  Downloading transformers-4.30.2-py3-none-any.whl (7.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: sentence-transformers==2.2.2 in /home/apohllo/miniconda3/envs/exaile/lib/python3.11/site-packages (from -r requirements.txt (line 10)) (2.2.2)\n",
      "Collecting elasticsearch==8.8.2 (from -r requirements.txt (line 11))\n",
      "  Downloading elasticsearch-8.8.2-py3-none-any.whl (393 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m393.9/393.9 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting jupyter==1.0.0 (from -r requirements.txt (line 12))\n",
      "  Using cached jupyter-1.0.0-py2.py3-none-any.whl (2.7 kB)\n",
      "Collecting datasets==2.13.1 (from -r requirements.txt (line 13))\n",
      "  Using cached datasets-2.13.1-py3-none-any.whl (486 kB)\n",
      "Collecting accelerate==0.20.3 (from -r requirements.txt (line 14))\n",
      "  Downloading accelerate-0.20.3-py3-none-any.whl (227 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.6/227.6 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting evaluate==0.4.0 (from -r requirements.txt (line 15))\n",
      "  Using cached evaluate-0.4.0-py3-none-any.whl (81 kB)\n",
      "Collecting neptune==1.3.2 (from -r requirements.txt (line 16))\n",
      "  Downloading neptune-1.3.2-py3-none-any.whl (455 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m455.3/455.3 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting jupytext==1.14.7 (from -r requirements.txt (line 17))\n",
      "  Downloading jupytext-1.14.7-py3-none-any.whl (299 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m299.1/299.1 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting jupyterlab==4.0.2 (from -r requirements.txt (line 18))\n",
      "  Downloading jupyterlab-4.0.2-py3-none-any.whl (9.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.2/9.2 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting xformers==0.0.20 (from -r requirements.txt (line 19))\n",
      "  Downloading xformers-0.0.20-cp311-cp311-manylinux2014_x86_64.whl (109.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m109.1/109.1 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /home/apohllo/miniconda3/envs/exaile/lib/python3.11/site-packages (from requests==2.31.0->-r requirements.txt (line 1)) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/apohllo/miniconda3/envs/exaile/lib/python3.11/site-packages (from requests==2.31.0->-r requirements.txt (line 1)) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/apohllo/miniconda3/envs/exaile/lib/python3.11/site-packages (from requests==2.31.0->-r requirements.txt (line 1)) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/apohllo/miniconda3/envs/exaile/lib/python3.11/site-packages (from requests==2.31.0->-r requirements.txt (line 1)) (2023.5.7)\n",
      "Requirement already satisfied: emoji in /home/apohllo/miniconda3/envs/exaile/lib/python3.11/site-packages (from stanza==1.5.0->-r requirements.txt (line 3)) (2.2.0)\n",
      "Requirement already satisfied: numpy in /home/apohllo/miniconda3/envs/exaile/lib/python3.11/site-packages (from stanza==1.5.0->-r requirements.txt (line 3)) (1.24.3)\n",
      "Requirement already satisfied: protobuf in /home/apohllo/miniconda3/envs/exaile/lib/python3.11/site-packages (from stanza==1.5.0->-r requirements.txt (line 3)) (4.23.1)\n",
      "Requirement already satisfied: six in /home/apohllo/miniconda3/envs/exaile/lib/python3.11/site-packages (from stanza==1.5.0->-r requirements.txt (line 3)) (1.16.0)\n",
      "Requirement already satisfied: torch>=1.3.0 in /home/apohllo/miniconda3/envs/exaile/lib/python3.11/site-packages (from stanza==1.5.0->-r requirements.txt (line 3)) (2.0.1)\n",
      "Requirement already satisfied: click in /home/apohllo/miniconda3/envs/exaile/lib/python3.11/site-packages (from nltk==3.8.1->-r requirements.txt (line 4)) (8.1.3)\n",
      "Requirement already satisfied: joblib in /home/apohllo/miniconda3/envs/exaile/lib/python3.11/site-packages (from nltk==3.8.1->-r requirements.txt (line 4)) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/apohllo/miniconda3/envs/exaile/lib/python3.11/site-packages (from nltk==3.8.1->-r requirements.txt (line 4)) (2023.5.5)\n",
      "Requirement already satisfied: pydantic>=1.9.1 in /home/apohllo/miniconda3/envs/exaile/lib/python3.11/site-packages (from inflect==7.0.0->-r requirements.txt (line 5)) (1.10.8)\n",
      "Requirement already satisfied: typing-extensions in /home/apohllo/miniconda3/envs/exaile/lib/python3.11/site-packages (from inflect==7.0.0->-r requirements.txt (line 5)) (4.6.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/apohllo/miniconda3/envs/exaile/lib/python3.11/site-packages (from matplotlib==3.7.2->-r requirements.txt (line 6)) (1.0.7)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/apohllo/miniconda3/envs/exaile/lib/python3.11/site-packages (from matplotlib==3.7.2->-r requirements.txt (line 6)) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/apohllo/miniconda3/envs/exaile/lib/python3.11/site-packages (from matplotlib==3.7.2->-r requirements.txt (line 6)) (4.40.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/apohllo/miniconda3/envs/exaile/lib/python3.11/site-packages (from matplotlib==3.7.2->-r requirements.txt (line 6)) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/apohllo/miniconda3/envs/exaile/lib/python3.11/site-packages (from matplotlib==3.7.2->-r requirements.txt (line 6)) (23.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /home/apohllo/miniconda3/envs/exaile/lib/python3.11/site-packages (from matplotlib==3.7.2->-r requirements.txt (line 6)) (9.5.0)\n",
      "Requirement already satisfied: pyparsing<3.1,>=2.3.1 in /home/apohllo/miniconda3/envs/exaile/lib/python3.11/site-packages (from matplotlib==3.7.2->-r requirements.txt (line 6)) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/apohllo/miniconda3/envs/exaile/lib/python3.11/site-packages (from matplotlib==3.7.2->-r requirements.txt (line 6)) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/apohllo/miniconda3/envs/exaile/lib/python3.11/site-packages (from pandas==2.0.3->-r requirements.txt (line 7)) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /home/apohllo/miniconda3/envs/exaile/lib/python3.11/site-packages (from pandas==2.0.3->-r requirements.txt (line 7)) (2023.3)\n",
      "Requirement already satisfied: scipy>=1.5.0 in /home/apohllo/miniconda3/envs/exaile/lib/python3.11/site-packages (from scikit-learn==1.3.0->-r requirements.txt (line 8)) (1.10.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/apohllo/miniconda3/envs/exaile/lib/python3.11/site-packages (from scikit-learn==1.3.0->-r requirements.txt (line 8)) (3.1.0)\n",
      "Requirement already satisfied: filelock in /home/apohllo/miniconda3/envs/exaile/lib/python3.11/site-packages (from transformers==4.30.2->-r requirements.txt (line 9)) (3.12.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /home/apohllo/miniconda3/envs/exaile/lib/python3.11/site-packages (from transformers==4.30.2->-r requirements.txt (line 9)) (0.15.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/apohllo/miniconda3/envs/exaile/lib/python3.11/site-packages (from transformers==4.30.2->-r requirements.txt (line 9)) (6.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /home/apohllo/miniconda3/envs/exaile/lib/python3.11/site-packages (from transformers==4.30.2->-r requirements.txt (line 9)) (0.13.3)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /home/apohllo/miniconda3/envs/exaile/lib/python3.11/site-packages (from transformers==4.30.2->-r requirements.txt (line 9)) (0.3.1)\n",
      "Requirement already satisfied: torchvision in /home/apohllo/miniconda3/envs/exaile/lib/python3.11/site-packages (from sentence-transformers==2.2.2->-r requirements.txt (line 10)) (0.15.2)\n",
      "Requirement already satisfied: sentencepiece in /home/apohllo/miniconda3/envs/exaile/lib/python3.11/site-packages (from sentence-transformers==2.2.2->-r requirements.txt (line 10)) (0.1.99)\n",
      "Requirement already satisfied: elastic-transport<9,>=8 in /home/apohllo/miniconda3/envs/exaile/lib/python3.11/site-packages (from elasticsearch==8.8.2->-r requirements.txt (line 11)) (8.4.0)\n",
      "Collecting notebook (from jupyter==1.0.0->-r requirements.txt (line 12))\n",
      "  Downloading notebook-7.0.4-py3-none-any.whl (4.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0mm\n",
      "\u001b[?25hCollecting qtconsole (from jupyter==1.0.0->-r requirements.txt (line 12))\n",
      "  Downloading qtconsole-5.4.4-py3-none-any.whl (121 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.9/121.9 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting jupyter-console (from jupyter==1.0.0->-r requirements.txt (line 12))\n",
      "  Downloading jupyter_console-6.6.3-py3-none-any.whl (24 kB)\n",
      "Collecting nbconvert (from jupyter==1.0.0->-r requirements.txt (line 12))\n",
      "  Downloading nbconvert-7.8.0-py3-none-any.whl (254 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m254.9/254.9 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: ipykernel in /home/apohllo/miniconda3/envs/exaile/lib/python3.11/site-packages (from jupyter==1.0.0->-r requirements.txt (line 12)) (6.23.1)\n",
      "Collecting ipywidgets (from jupyter==1.0.0->-r requirements.txt (line 12))\n",
      "  Downloading ipywidgets-8.1.1-py3-none-any.whl (139 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.4/139.4 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pyarrow>=8.0.0 (from datasets==2.13.1->-r requirements.txt (line 13))\n",
      "  Downloading pyarrow-13.0.0-cp311-cp311-manylinux_2_28_x86_64.whl (40.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.0/40.0 MB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting dill<0.3.7,>=0.3.0 (from datasets==2.13.1->-r requirements.txt (line 13))\n",
      "  Using cached dill-0.3.6-py3-none-any.whl (110 kB)\n",
      "Collecting xxhash (from datasets==2.13.1->-r requirements.txt (line 13))\n",
      "  Downloading xxhash-3.3.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting multiprocess (from datasets==2.13.1->-r requirements.txt (line 13))\n",
      "  Downloading multiprocess-0.70.15-py311-none-any.whl (135 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m135.4/135.4 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: fsspec[http]>=2021.11.1 in /home/apohllo/miniconda3/envs/exaile/lib/python3.11/site-packages (from datasets==2.13.1->-r requirements.txt (line 13)) (2023.6.0)\n",
      "Collecting aiohttp (from datasets==2.13.1->-r requirements.txt (line 13))\n",
      "  Using cached aiohttp-3.8.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
      "Requirement already satisfied: psutil in /home/apohllo/miniconda3/envs/exaile/lib/python3.11/site-packages (from accelerate==0.20.3->-r requirements.txt (line 14)) (5.9.5)\n",
      "Collecting responses<0.19 (from evaluate==0.4.0->-r requirements.txt (line 15))\n",
      "  Using cached responses-0.18.0-py3-none-any.whl (38 kB)\n",
      "Collecting GitPython>=2.0.8 (from neptune==1.3.2->-r requirements.txt (line 16))\n",
      "  Using cached GitPython-3.1.37-py3-none-any.whl (190 kB)\n",
      "Collecting PyJWT (from neptune==1.3.2->-r requirements.txt (line 16))\n",
      "  Downloading PyJWT-2.8.0-py3-none-any.whl (22 kB)\n",
      "Collecting backoff (from neptune==1.3.2->-r requirements.txt (line 16))\n",
      "  Downloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
      "Collecting boto3>=1.16.0 (from neptune==1.3.2->-r requirements.txt (line 16))\n",
      "  Downloading boto3-1.28.55-py3-none-any.whl (135 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m135.8/135.8 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting bravado<12.0.0,>=11.0.0 (from neptune==1.3.2->-r requirements.txt (line 16))\n",
      "  Using cached bravado-11.0.3-py2.py3-none-any.whl (38 kB)\n",
      "Collecting future>=0.17.1 (from neptune==1.3.2->-r requirements.txt (line 16))\n",
      "  Using cached future-0.18.3.tar.gz (840 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting oauthlib>=2.1.0 (from neptune==1.3.2->-r requirements.txt (line 16))\n",
      "  Using cached oauthlib-3.2.2-py3-none-any.whl (151 kB)\n",
      "Collecting requests-oauthlib>=1.0.0 (from neptune==1.3.2->-r requirements.txt (line 16))\n",
      "  Using cached requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
      "Collecting swagger-spec-validator>=2.7.4 (from neptune==1.3.2->-r requirements.txt (line 16))\n",
      "  Using cached swagger_spec_validator-3.0.3-py2.py3-none-any.whl (27 kB)\n",
      "Collecting websocket-client!=1.0.0,>=0.35.0 (from neptune==1.3.2->-r requirements.txt (line 16))\n",
      "  Downloading websocket_client-1.6.3-py3-none-any.whl (57 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.3/57.3 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting nbformat (from jupytext==1.14.7->-r requirements.txt (line 17))\n",
      "  Downloading nbformat-5.9.2-py3-none-any.whl (77 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.6/77.6 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting toml (from jupytext==1.14.7->-r requirements.txt (line 17))\n",
      "  Using cached toml-0.10.2-py2.py3-none-any.whl (16 kB)\n",
      "Collecting markdown-it-py>=1.0.0 (from jupytext==1.14.7->-r requirements.txt (line 17))\n",
      "  Using cached markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "Collecting mdit-py-plugins (from jupytext==1.14.7->-r requirements.txt (line 17))\n",
      "  Using cached mdit_py_plugins-0.4.0-py3-none-any.whl (54 kB)\n",
      "Collecting async-lru>=1.0.0 (from jupyterlab==4.0.2->-r requirements.txt (line 18))\n",
      "  Downloading async_lru-2.0.4-py3-none-any.whl (6.1 kB)\n",
      "Requirement already satisfied: jinja2>=3.0.3 in /home/apohllo/miniconda3/envs/exaile/lib/python3.11/site-packages (from jupyterlab==4.0.2->-r requirements.txt (line 18)) (3.1.2)\n",
      "Requirement already satisfied: jupyter-core in /home/apohllo/miniconda3/envs/exaile/lib/python3.11/site-packages (from jupyterlab==4.0.2->-r requirements.txt (line 18)) (5.3.0)\n",
      "Collecting jupyter-lsp>=2.0.0 (from jupyterlab==4.0.2->-r requirements.txt (line 18))\n",
      "  Downloading jupyter_lsp-2.2.0-py3-none-any.whl (65 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.0/66.0 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting jupyter-server<3,>=2.4.0 (from jupyterlab==4.0.2->-r requirements.txt (line 18))\n",
      "  Downloading jupyter_server-2.7.3-py3-none-any.whl (375 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m375.6/375.6 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting jupyterlab-server<3,>=2.19.0 (from jupyterlab==4.0.2->-r requirements.txt (line 18))\n",
      "  Downloading jupyterlab_server-2.25.0-py3-none-any.whl (57 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.2/57.2 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting notebook-shim>=0.2 (from jupyterlab==4.0.2->-r requirements.txt (line 18))\n",
      "  Downloading notebook_shim-0.2.3-py3-none-any.whl (13 kB)\n",
      "Requirement already satisfied: tornado>=6.2.0 in /home/apohllo/miniconda3/envs/exaile/lib/python3.11/site-packages (from jupyterlab==4.0.2->-r requirements.txt (line 18)) (6.3.2)\n",
      "Requirement already satisfied: traitlets in /home/apohllo/miniconda3/envs/exaile/lib/python3.11/site-packages (from jupyterlab==4.0.2->-r requirements.txt (line 18)) (5.9.0)\n",
      "Collecting pyre-extensions==0.0.29 (from xformers==0.0.20->-r requirements.txt (line 19))\n",
      "  Downloading pyre_extensions-0.0.29-py3-none-any.whl (12 kB)\n",
      "Collecting typing-inspect (from pyre-extensions==0.0.29->xformers==0.0.20->-r requirements.txt (line 19))\n",
      "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Requirement already satisfied: sympy in /home/apohllo/miniconda3/envs/exaile/lib/python3.11/site-packages (from torch>=1.3.0->stanza==1.5.0->-r requirements.txt (line 3)) (1.12)\n",
      "Requirement already satisfied: networkx in /home/apohllo/miniconda3/envs/exaile/lib/python3.11/site-packages (from torch>=1.3.0->stanza==1.5.0->-r requirements.txt (line 3)) (3.1)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /home/apohllo/miniconda3/envs/exaile/lib/python3.11/site-packages (from torch>=1.3.0->stanza==1.5.0->-r requirements.txt (line 3)) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /home/apohllo/miniconda3/envs/exaile/lib/python3.11/site-packages (from torch>=1.3.0->stanza==1.5.0->-r requirements.txt (line 3)) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in /home/apohllo/miniconda3/envs/exaile/lib/python3.11/site-packages (from torch>=1.3.0->stanza==1.5.0->-r requirements.txt (line 3)) (11.7.101)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /home/apohllo/miniconda3/envs/exaile/lib/python3.11/site-packages (from torch>=1.3.0->stanza==1.5.0->-r requirements.txt (line 3)) (8.5.0.96)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /home/apohllo/miniconda3/envs/exaile/lib/python3.11/site-packages (from torch>=1.3.0->stanza==1.5.0->-r requirements.txt (line 3)) (11.10.3.66)\n",
      "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /home/apohllo/miniconda3/envs/exaile/lib/python3.11/site-packages (from torch>=1.3.0->stanza==1.5.0->-r requirements.txt (line 3)) (10.9.0.58)\n",
      "Requirement already satisfied: nvidia-curand-cu11==10.2.10.91 in /home/apohllo/miniconda3/envs/exaile/lib/python3.11/site-packages (from torch>=1.3.0->stanza==1.5.0->-r requirements.txt (line 3)) (10.2.10.91)\n",
      "Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in /home/apohllo/miniconda3/envs/exaile/lib/python3.11/site-packages (from torch>=1.3.0->stanza==1.5.0->-r requirements.txt (line 3)) (11.4.0.1)\n",
      "Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in /home/apohllo/miniconda3/envs/exaile/lib/python3.11/site-packages (from torch>=1.3.0->stanza==1.5.0->-r requirements.txt (line 3)) (11.7.4.91)\n",
      "Requirement already satisfied: nvidia-nccl-cu11==2.14.3 in /home/apohllo/miniconda3/envs/exaile/lib/python3.11/site-packages (from torch>=1.3.0->stanza==1.5.0->-r requirements.txt (line 3)) (2.14.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu11==11.7.91 in /home/apohllo/miniconda3/envs/exaile/lib/python3.11/site-packages (from torch>=1.3.0->stanza==1.5.0->-r requirements.txt (line 3)) (11.7.91)\n",
      "Requirement already satisfied: triton==2.0.0 in /home/apohllo/miniconda3/envs/exaile/lib/python3.11/site-packages (from torch>=1.3.0->stanza==1.5.0->-r requirements.txt (line 3)) (2.0.0)\n",
      "Requirement already satisfied: setuptools in /home/apohllo/miniconda3/envs/exaile/lib/python3.11/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.3.0->stanza==1.5.0->-r requirements.txt (line 3)) (67.7.2)\n",
      "Requirement already satisfied: wheel in /home/apohllo/miniconda3/envs/exaile/lib/python3.11/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.3.0->stanza==1.5.0->-r requirements.txt (line 3)) (0.40.0)\n",
      "Requirement already satisfied: cmake in /home/apohllo/miniconda3/envs/exaile/lib/python3.11/site-packages (from triton==2.0.0->torch>=1.3.0->stanza==1.5.0->-r requirements.txt (line 3)) (3.26.3)\n",
      "Requirement already satisfied: lit in /home/apohllo/miniconda3/envs/exaile/lib/python3.11/site-packages (from triton==2.0.0->torch>=1.3.0->stanza==1.5.0->-r requirements.txt (line 3)) (16.0.5)\n",
      "Collecting botocore<1.32.0,>=1.31.55 (from boto3>=1.16.0->neptune==1.3.2->-r requirements.txt (line 16))\n",
      "  Downloading botocore-1.31.55-py3-none-any.whl (11.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.2/11.2 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting jmespath<2.0.0,>=0.7.1 (from boto3>=1.16.0->neptune==1.3.2->-r requirements.txt (line 16))\n",
      "  Using cached jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
      "Collecting s3transfer<0.8.0,>=0.7.0 (from boto3>=1.16.0->neptune==1.3.2->-r requirements.txt (line 16))\n",
      "  Downloading s3transfer-0.7.0-py3-none-any.whl (79 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.8/79.8 kB\u001b[0m \u001b[31m644.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting bravado-core>=5.16.1 (from bravado<12.0.0,>=11.0.0->neptune==1.3.2->-r requirements.txt (line 16))\n",
      "  Downloading bravado_core-6.1.0-py2.py3-none-any.whl (67 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.7/67.7 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting msgpack (from bravado<12.0.0,>=11.0.0->neptune==1.3.2->-r requirements.txt (line 16))\n",
      "  Downloading msgpack-1.0.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (558 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m558.0/558.0 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting simplejson (from bravado<12.0.0,>=11.0.0->neptune==1.3.2->-r requirements.txt (line 16))\n",
      "  Downloading simplejson-3.19.1-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (144 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m144.7/144.7 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting monotonic (from bravado<12.0.0,>=11.0.0->neptune==1.3.2->-r requirements.txt (line 16))\n",
      "  Using cached monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\n",
      "Collecting attrs>=17.3.0 (from aiohttp->datasets==2.13.1->-r requirements.txt (line 13))\n",
      "  Using cached attrs-23.1.0-py3-none-any.whl (61 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp->datasets==2.13.1->-r requirements.txt (line 13))\n",
      "  Using cached multidict-6.0.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (117 kB)\n",
      "Collecting async-timeout<5.0,>=4.0.0a3 (from aiohttp->datasets==2.13.1->-r requirements.txt (line 13))\n",
      "  Downloading async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\n",
      "Collecting yarl<2.0,>=1.0 (from aiohttp->datasets==2.13.1->-r requirements.txt (line 13))\n",
      "  Using cached yarl-1.9.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (282 kB)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp->datasets==2.13.1->-r requirements.txt (line 13))\n",
      "  Using cached frozenlist-1.4.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (250 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp->datasets==2.13.1->-r requirements.txt (line 13))\n",
      "  Using cached aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Collecting gitdb<5,>=4.0.1 (from GitPython>=2.0.8->neptune==1.3.2->-r requirements.txt (line 16))\n",
      "  Using cached gitdb-4.0.10-py3-none-any.whl (62 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/apohllo/miniconda3/envs/exaile/lib/python3.11/site-packages (from jinja2>=3.0.3->jupyterlab==4.0.2->-r requirements.txt (line 18)) (2.1.2)\n",
      "Collecting anyio>=3.1.0 (from jupyter-server<3,>=2.4.0->jupyterlab==4.0.2->-r requirements.txt (line 18))\n",
      "  Downloading anyio-4.0.0-py3-none-any.whl (83 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.1/83.1 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting argon2-cffi (from jupyter-server<3,>=2.4.0->jupyterlab==4.0.2->-r requirements.txt (line 18))\n",
      "  Downloading argon2_cffi-23.1.0-py3-none-any.whl (15 kB)\n",
      "Requirement already satisfied: jupyter-client>=7.4.4 in /home/apohllo/miniconda3/envs/exaile/lib/python3.11/site-packages (from jupyter-server<3,>=2.4.0->jupyterlab==4.0.2->-r requirements.txt (line 18)) (8.2.0)\n",
      "Collecting jupyter-events>=0.6.0 (from jupyter-server<3,>=2.4.0->jupyterlab==4.0.2->-r requirements.txt (line 18))\n",
      "  Downloading jupyter_events-0.7.0-py3-none-any.whl (18 kB)\n",
      "Collecting jupyter-server-terminals (from jupyter-server<3,>=2.4.0->jupyterlab==4.0.2->-r requirements.txt (line 18))\n",
      "  Downloading jupyter_server_terminals-0.4.4-py3-none-any.whl (13 kB)\n",
      "Collecting overrides (from jupyter-server<3,>=2.4.0->jupyterlab==4.0.2->-r requirements.txt (line 18))\n",
      "  Downloading overrides-7.4.0-py3-none-any.whl (17 kB)\n",
      "Collecting prometheus-client (from jupyter-server<3,>=2.4.0->jupyterlab==4.0.2->-r requirements.txt (line 18))\n",
      "  Downloading prometheus_client-0.17.1-py3-none-any.whl (60 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pyzmq>=24 in /home/apohllo/miniconda3/envs/exaile/lib/python3.11/site-packages (from jupyter-server<3,>=2.4.0->jupyterlab==4.0.2->-r requirements.txt (line 18)) (25.0.2)\n",
      "Collecting send2trash>=1.8.2 (from jupyter-server<3,>=2.4.0->jupyterlab==4.0.2->-r requirements.txt (line 18))\n",
      "  Downloading Send2Trash-1.8.2-py3-none-any.whl (18 kB)\n",
      "Collecting terminado>=0.8.3 (from jupyter-server<3,>=2.4.0->jupyterlab==4.0.2->-r requirements.txt (line 18))\n",
      "  Downloading terminado-0.17.1-py3-none-any.whl (17 kB)\n",
      "Requirement already satisfied: platformdirs>=2.5 in /home/apohllo/miniconda3/envs/exaile/lib/python3.11/site-packages (from jupyter-core->jupyterlab==4.0.2->-r requirements.txt (line 18)) (3.5.1)\n",
      "Collecting babel>=2.10 (from jupyterlab-server<3,>=2.19.0->jupyterlab==4.0.2->-r requirements.txt (line 18))\n",
      "  Using cached Babel-2.12.1-py3-none-any.whl (10.1 MB)\n",
      "Collecting json5>=0.9.0 (from jupyterlab-server<3,>=2.19.0->jupyterlab==4.0.2->-r requirements.txt (line 18))\n",
      "  Downloading json5-0.9.14-py2.py3-none-any.whl (19 kB)\n",
      "Collecting jsonschema>=4.18.0 (from jupyterlab-server<3,>=2.19.0->jupyterlab==4.0.2->-r requirements.txt (line 18))\n",
      "  Using cached jsonschema-4.19.1-py3-none-any.whl (83 kB)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=1.0.0->jupytext==1.14.7->-r requirements.txt (line 17))\n",
      "  Using cached mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Collecting beautifulsoup4 (from nbconvert->jupyter==1.0.0->-r requirements.txt (line 12))\n",
      "  Downloading beautifulsoup4-4.12.2-py3-none-any.whl (142 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.0/143.0 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting bleach!=5.0.0 (from nbconvert->jupyter==1.0.0->-r requirements.txt (line 12))\n",
      "  Downloading bleach-6.0.0-py3-none-any.whl (162 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m162.5/162.5 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting defusedxml (from nbconvert->jupyter==1.0.0->-r requirements.txt (line 12))\n",
      "  Using cached defusedxml-0.7.1-py2.py3-none-any.whl (25 kB)\n",
      "Collecting jupyterlab-pygments (from nbconvert->jupyter==1.0.0->-r requirements.txt (line 12))\n",
      "  Using cached jupyterlab_pygments-0.2.2-py2.py3-none-any.whl (21 kB)\n",
      "Collecting mistune<4,>=2.0.3 (from nbconvert->jupyter==1.0.0->-r requirements.txt (line 12))\n",
      "  Downloading mistune-3.0.1-py3-none-any.whl (47 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.0/48.0 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting nbclient>=0.5.0 (from nbconvert->jupyter==1.0.0->-r requirements.txt (line 12))\n",
      "  Downloading nbclient-0.8.0-py3-none-any.whl (73 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.1/73.1 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pandocfilters>=1.4.1 (from nbconvert->jupyter==1.0.0->-r requirements.txt (line 12))\n",
      "  Using cached pandocfilters-1.5.0-py2.py3-none-any.whl (8.7 kB)\n",
      "Requirement already satisfied: pygments>=2.4.1 in /home/apohllo/miniconda3/envs/exaile/lib/python3.11/site-packages (from nbconvert->jupyter==1.0.0->-r requirements.txt (line 12)) (2.15.1)\n",
      "Collecting tinycss2 (from nbconvert->jupyter==1.0.0->-r requirements.txt (line 12))\n",
      "  Downloading tinycss2-1.2.1-py3-none-any.whl (21 kB)\n",
      "Collecting fastjsonschema (from nbformat->jupytext==1.14.7->-r requirements.txt (line 17))\n",
      "  Downloading fastjsonschema-2.18.0-py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: comm>=0.1.1 in /home/apohllo/miniconda3/envs/exaile/lib/python3.11/site-packages (from ipykernel->jupyter==1.0.0->-r requirements.txt (line 12)) (0.1.3)\n",
      "Requirement already satisfied: debugpy>=1.6.5 in /home/apohllo/miniconda3/envs/exaile/lib/python3.11/site-packages (from ipykernel->jupyter==1.0.0->-r requirements.txt (line 12)) (1.6.7)\n",
      "Requirement already satisfied: ipython>=7.23.1 in /home/apohllo/miniconda3/envs/exaile/lib/python3.11/site-packages (from ipykernel->jupyter==1.0.0->-r requirements.txt (line 12)) (8.13.2)\n",
      "Requirement already satisfied: matplotlib-inline>=0.1 in /home/apohllo/miniconda3/envs/exaile/lib/python3.11/site-packages (from ipykernel->jupyter==1.0.0->-r requirements.txt (line 12)) (0.1.6)\n",
      "Requirement already satisfied: nest-asyncio in /home/apohllo/miniconda3/envs/exaile/lib/python3.11/site-packages (from ipykernel->jupyter==1.0.0->-r requirements.txt (line 12)) (1.5.6)\n",
      "Collecting widgetsnbextension~=4.0.9 (from ipywidgets->jupyter==1.0.0->-r requirements.txt (line 12))\n",
      "  Downloading widgetsnbextension-4.0.9-py3-none-any.whl (2.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0mm\n",
      "\u001b[?25hCollecting jupyterlab-widgets~=3.0.9 (from ipywidgets->jupyter==1.0.0->-r requirements.txt (line 12))\n",
      "  Downloading jupyterlab_widgets-3.0.9-py3-none-any.whl (214 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m214.9/214.9 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: prompt-toolkit>=3.0.30 in /home/apohllo/miniconda3/envs/exaile/lib/python3.11/site-packages (from jupyter-console->jupyter==1.0.0->-r requirements.txt (line 12)) (3.0.38)\n",
      "INFO: pip is looking at multiple versions of multiprocess to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting multiprocess (from datasets==2.13.1->-r requirements.txt (line 13))\n",
      "  Using cached multiprocess-0.70.14-py310-none-any.whl (134 kB)\n",
      "Collecting ipython-genutils (from qtconsole->jupyter==1.0.0->-r requirements.txt (line 12))\n",
      "  Using cached ipython_genutils-0.2.0-py2.py3-none-any.whl (26 kB)\n",
      "Collecting qtpy>=2.4.0 (from qtconsole->jupyter==1.0.0->-r requirements.txt (line 12))\n",
      "  Downloading QtPy-2.4.0-py3-none-any.whl (93 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.4/93.4 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting sniffio>=1.1 (from anyio>=3.1.0->jupyter-server<3,>=2.4.0->jupyterlab==4.0.2->-r requirements.txt (line 18))\n",
      "  Using cached sniffio-1.3.0-py3-none-any.whl (10 kB)\n",
      "Collecting webencodings (from bleach!=5.0.0->nbconvert->jupyter==1.0.0->-r requirements.txt (line 12))\n",
      "  Using cached webencodings-0.5.1-py2.py3-none-any.whl (11 kB)\n",
      "Collecting jsonref (from bravado-core>=5.16.1->bravado<12.0.0,>=11.0.0->neptune==1.3.2->-r requirements.txt (line 16))\n",
      "  Using cached jsonref-1.1.0-py3-none-any.whl (9.4 kB)\n",
      "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->GitPython>=2.0.8->neptune==1.3.2->-r requirements.txt (line 16))\n",
      "  Using cached smmap-5.0.1-py3-none-any.whl (24 kB)\n",
      "Requirement already satisfied: backcall in /home/apohllo/miniconda3/envs/exaile/lib/python3.11/site-packages (from ipython>=7.23.1->ipykernel->jupyter==1.0.0->-r requirements.txt (line 12)) (0.2.0)\n",
      "Requirement already satisfied: decorator in /home/apohllo/miniconda3/envs/exaile/lib/python3.11/site-packages (from ipython>=7.23.1->ipykernel->jupyter==1.0.0->-r requirements.txt (line 12)) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /home/apohllo/miniconda3/envs/exaile/lib/python3.11/site-packages (from ipython>=7.23.1->ipykernel->jupyter==1.0.0->-r requirements.txt (line 12)) (0.18.2)\n",
      "Requirement already satisfied: pickleshare in /home/apohllo/miniconda3/envs/exaile/lib/python3.11/site-packages (from ipython>=7.23.1->ipykernel->jupyter==1.0.0->-r requirements.txt (line 12)) (0.7.5)\n",
      "Requirement already satisfied: stack-data in /home/apohllo/miniconda3/envs/exaile/lib/python3.11/site-packages (from ipython>=7.23.1->ipykernel->jupyter==1.0.0->-r requirements.txt (line 12)) (0.6.2)\n",
      "Requirement already satisfied: pexpect>4.3 in /home/apohllo/miniconda3/envs/exaile/lib/python3.11/site-packages (from ipython>=7.23.1->ipykernel->jupyter==1.0.0->-r requirements.txt (line 12)) (4.8.0)\n",
      "Collecting jsonschema-specifications>=2023.03.6 (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.19.0->jupyterlab==4.0.2->-r requirements.txt (line 18))\n",
      "  Using cached jsonschema_specifications-2023.7.1-py3-none-any.whl (17 kB)\n",
      "Collecting referencing>=0.28.4 (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.19.0->jupyterlab==4.0.2->-r requirements.txt (line 18))\n",
      "  Using cached referencing-0.30.2-py3-none-any.whl (25 kB)\n",
      "Collecting rpds-py>=0.7.1 (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.19.0->jupyterlab==4.0.2->-r requirements.txt (line 18))\n",
      "  Downloading rpds_py-0.10.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting python-json-logger>=2.0.4 (from jupyter-events>=0.6.0->jupyter-server<3,>=2.4.0->jupyterlab==4.0.2->-r requirements.txt (line 18))\n",
      "  Downloading python_json_logger-2.0.7-py3-none-any.whl (8.1 kB)\n",
      "Collecting rfc3339-validator (from jupyter-events>=0.6.0->jupyter-server<3,>=2.4.0->jupyterlab==4.0.2->-r requirements.txt (line 18))\n",
      "  Using cached rfc3339_validator-0.1.4-py2.py3-none-any.whl (3.5 kB)\n",
      "Collecting rfc3986-validator>=0.1.1 (from jupyter-events>=0.6.0->jupyter-server<3,>=2.4.0->jupyterlab==4.0.2->-r requirements.txt (line 18))\n",
      "  Downloading rfc3986_validator-0.1.1-py2.py3-none-any.whl (4.2 kB)\n",
      "Requirement already satisfied: wcwidth in /home/apohllo/miniconda3/envs/exaile/lib/python3.11/site-packages (from prompt-toolkit>=3.0.30->jupyter-console->jupyter==1.0.0->-r requirements.txt (line 12)) (0.2.6)\n",
      "Requirement already satisfied: ptyprocess in /home/apohllo/miniconda3/envs/exaile/lib/python3.11/site-packages (from terminado>=0.8.3->jupyter-server<3,>=2.4.0->jupyterlab==4.0.2->-r requirements.txt (line 18)) (0.7.0)\n",
      "Collecting argon2-cffi-bindings (from argon2-cffi->jupyter-server<3,>=2.4.0->jupyterlab==4.0.2->-r requirements.txt (line 18))\n",
      "  Using cached argon2_cffi_bindings-21.2.0-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (86 kB)\n",
      "Collecting soupsieve>1.2 (from beautifulsoup4->nbconvert->jupyter==1.0.0->-r requirements.txt (line 12))\n",
      "  Downloading soupsieve-2.5-py3-none-any.whl (36 kB)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/apohllo/miniconda3/envs/exaile/lib/python3.11/site-packages (from sympy->torch>=1.3.0->stanza==1.5.0->-r requirements.txt (line 3)) (1.3.0)\n",
      "Collecting mypy-extensions>=0.3.0 (from typing-inspect->pyre-extensions==0.0.29->xformers==0.0.20->-r requirements.txt (line 19))\n",
      "  Using cached mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /home/apohllo/miniconda3/envs/exaile/lib/python3.11/site-packages (from jedi>=0.16->ipython>=7.23.1->ipykernel->jupyter==1.0.0->-r requirements.txt (line 12)) (0.8.3)\n",
      "Collecting fqdn (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.19.0->jupyterlab==4.0.2->-r requirements.txt (line 18))\n",
      "  Downloading fqdn-1.5.1-py3-none-any.whl (9.1 kB)\n",
      "Collecting isoduration (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.19.0->jupyterlab==4.0.2->-r requirements.txt (line 18))\n",
      "  Downloading isoduration-20.11.0-py3-none-any.whl (11 kB)\n",
      "Collecting jsonpointer>1.13 (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.19.0->jupyterlab==4.0.2->-r requirements.txt (line 18))\n",
      "  Downloading jsonpointer-2.4-py2.py3-none-any.whl (7.8 kB)\n",
      "Collecting uri-template (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.19.0->jupyterlab==4.0.2->-r requirements.txt (line 18))\n",
      "  Downloading uri_template-1.3.0-py3-none-any.whl (11 kB)\n",
      "Collecting webcolors>=1.11 (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.19.0->jupyterlab==4.0.2->-r requirements.txt (line 18))\n",
      "  Using cached webcolors-1.13-py3-none-any.whl (14 kB)\n",
      "Collecting rfc3987 (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.19.0->jupyterlab==4.0.2->-r requirements.txt (line 18))\n",
      "  Using cached rfc3987-1.3.8-py2.py3-none-any.whl (13 kB)\n",
      "Collecting cffi>=1.0.1 (from argon2-cffi-bindings->argon2-cffi->jupyter-server<3,>=2.4.0->jupyterlab==4.0.2->-r requirements.txt (line 18))\n",
      "  Using cached cffi-1.15.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (462 kB)\n",
      "Requirement already satisfied: executing>=1.2.0 in /home/apohllo/miniconda3/envs/exaile/lib/python3.11/site-packages (from stack-data->ipython>=7.23.1->ipykernel->jupyter==1.0.0->-r requirements.txt (line 12)) (1.2.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /home/apohllo/miniconda3/envs/exaile/lib/python3.11/site-packages (from stack-data->ipython>=7.23.1->ipykernel->jupyter==1.0.0->-r requirements.txt (line 12)) (2.2.1)\n",
      "Requirement already satisfied: pure-eval in /home/apohllo/miniconda3/envs/exaile/lib/python3.11/site-packages (from stack-data->ipython>=7.23.1->ipykernel->jupyter==1.0.0->-r requirements.txt (line 12)) (0.2.2)\n",
      "Collecting pycparser (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->jupyter-server<3,>=2.4.0->jupyterlab==4.0.2->-r requirements.txt (line 18))\n",
      "  Using cached pycparser-2.21-py2.py3-none-any.whl (118 kB)\n",
      "Collecting arrow>=0.15.0 (from isoduration->jsonschema>=4.18.0->jupyterlab-server<3,>=2.19.0->jupyterlab==4.0.2->-r requirements.txt (line 18))\n",
      "  Downloading arrow-1.2.3-py3-none-any.whl (66 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.4/66.4 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: future\n",
      "  Building wheel for future (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for future: filename=future-0.18.3-py3-none-any.whl size=492022 sha256=72b64e1d7797b7eb4d3023ef62385ff22e0a11ad538574cbbb33d2527b10833e\n",
      "  Stored in directory: /home/apohllo/.cache/pip/wheels/da/19/ca/9d8c44cd311a955509d7e13da3f0bea42400c469ef825b580b\n",
      "Successfully built future\n",
      "Installing collected packages: webencodings, rfc3987, monotonic, json5, ipython-genutils, fastjsonschema, xxhash, widgetsnbextension, websocket-client, webcolors, uri-template, toml, tinycss2, terminado, soupsieve, sniffio, smmap, simplejson, send2trash, rpds-py, rfc3986-validator, rfc3339-validator, qtpy, python-json-logger, PyJWT, pycparser, pyarrow, prometheus-client, pandocfilters, overrides, oauthlib, mypy-extensions, multidict, msgpack, mistune, mdurl, jupyterlab-widgets, jupyterlab-pygments, jsonref, jsonpointer, jmespath, future, frozenlist, fqdn, dill, defusedxml, bleach, backoff, babel, attrs, async-timeout, async-lru, yarl, typing-inspect, scikit-learn, responses, requests-oauthlib, referencing, pandas, multiprocess, matplotlib, markdown-it-py, jupyter-server-terminals, inflect, gitdb, elasticsearch, cffi, botocore, beautifulsoup4, arrow, anyio, aiosignal, transformers, s3transfer, pyre-extensions, mdit-py-plugins, jsonschema-specifications, isoduration, GitPython, argon2-cffi-bindings, aiohttp, jsonschema, ipywidgets, boto3, argon2-cffi, swagger-spec-validator, qtconsole, nbformat, jupyter-console, datasets, nbclient, jupytext, jupyter-events, evaluate, bravado-core, nbconvert, bravado, neptune, jupyter-server, notebook-shim, jupyterlab-server, jupyter-lsp, jupyterlab, notebook, jupyter, xformers, accelerate\n",
      "  Attempting uninstall: scikit-learn\n",
      "    Found existing installation: scikit-learn 1.2.2\n",
      "    Uninstalling scikit-learn-1.2.2:\n",
      "      Successfully uninstalled scikit-learn-1.2.2\n",
      "  Attempting uninstall: pandas\n",
      "    Found existing installation: pandas 2.0.2\n",
      "    Uninstalling pandas-2.0.2:\n",
      "      Successfully uninstalled pandas-2.0.2\n",
      "  Attempting uninstall: matplotlib\n",
      "    Found existing installation: matplotlib 3.7.1\n",
      "    Uninstalling matplotlib-3.7.1:\n",
      "      Successfully uninstalled matplotlib-3.7.1\n",
      "  Attempting uninstall: inflect\n",
      "    Found existing installation: inflect 6.0.4\n",
      "    Uninstalling inflect-6.0.4:\n",
      "      Successfully uninstalled inflect-6.0.4\n",
      "  Attempting uninstall: elasticsearch\n",
      "    Found existing installation: elasticsearch 8.8.0\n",
      "    Uninstalling elasticsearch-8.8.0:\n",
      "      Successfully uninstalled elasticsearch-8.8.0\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.30.1\n",
      "    Uninstalling transformers-4.30.1:\n",
      "      Successfully uninstalled transformers-4.30.1\n",
      "Successfully installed GitPython-3.1.37 PyJWT-2.8.0 accelerate-0.20.3 aiohttp-3.8.5 aiosignal-1.3.1 anyio-4.0.0 argon2-cffi-23.1.0 argon2-cffi-bindings-21.2.0 arrow-1.2.3 async-lru-2.0.4 async-timeout-4.0.3 attrs-23.1.0 babel-2.12.1 backoff-2.2.1 beautifulsoup4-4.12.2 bleach-6.0.0 boto3-1.28.55 botocore-1.31.55 bravado-11.0.3 bravado-core-6.1.0 cffi-1.15.1 datasets-2.13.1 defusedxml-0.7.1 dill-0.3.6 elasticsearch-8.8.2 evaluate-0.4.0 fastjsonschema-2.18.0 fqdn-1.5.1 frozenlist-1.4.0 future-0.18.3 gitdb-4.0.10 inflect-7.0.0 ipython-genutils-0.2.0 ipywidgets-8.1.1 isoduration-20.11.0 jmespath-1.0.1 json5-0.9.14 jsonpointer-2.4 jsonref-1.1.0 jsonschema-4.19.1 jsonschema-specifications-2023.7.1 jupyter-1.0.0 jupyter-console-6.6.3 jupyter-events-0.7.0 jupyter-lsp-2.2.0 jupyter-server-2.7.3 jupyter-server-terminals-0.4.4 jupyterlab-4.0.2 jupyterlab-pygments-0.2.2 jupyterlab-server-2.25.0 jupyterlab-widgets-3.0.9 jupytext-1.14.7 markdown-it-py-3.0.0 matplotlib-3.7.2 mdit-py-plugins-0.4.0 mdurl-0.1.2 mistune-3.0.1 monotonic-1.6 msgpack-1.0.6 multidict-6.0.4 multiprocess-0.70.14 mypy-extensions-1.0.0 nbclient-0.8.0 nbconvert-7.8.0 nbformat-5.9.2 neptune-1.3.2 notebook-7.0.4 notebook-shim-0.2.3 oauthlib-3.2.2 overrides-7.4.0 pandas-2.0.3 pandocfilters-1.5.0 prometheus-client-0.17.1 pyarrow-13.0.0 pycparser-2.21 pyre-extensions-0.0.29 python-json-logger-2.0.7 qtconsole-5.4.4 qtpy-2.4.0 referencing-0.30.2 requests-oauthlib-1.3.1 responses-0.18.0 rfc3339-validator-0.1.4 rfc3986-validator-0.1.1 rfc3987-1.3.8 rpds-py-0.10.3 s3transfer-0.7.0 scikit-learn-1.3.0 send2trash-1.8.2 simplejson-3.19.1 smmap-5.0.1 sniffio-1.3.0 soupsieve-2.5 swagger-spec-validator-3.0.3 terminado-0.17.1 tinycss2-1.2.1 toml-0.10.2 transformers-4.30.2 typing-inspect-0.9.0 uri-template-1.3.0 webcolors-1.13 webencodings-0.5.1 websocket-client-1.6.3 widgetsnbextension-4.0.9 xformers-0.0.20 xxhash-3.3.0 yarl-1.9.2\n"
     ]
    }
   ],
   "source": [
    "! pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take into account only decisions related to articles 44-46 of GDPR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import tqdm\n",
    "import re\n",
    "import os\n",
    "\n",
    "API_ENPOINT=\"https://gdprhub.eu/api.php\"\n",
    "\n",
    "category_titles = [(\"Category:Article_44_GDPR\",\"44\"),  \n",
    "                   (\"Category:Article_45_GDPR\",\"45\"),\n",
    "                   (\"Category:Article_45(1)_GDPR\",\"45\"),\n",
    "                   (\"Category:Article_45(2)_GDPR\",\"45\"),\n",
    "                   (\"Category:Article_45(3)_GDPR\",\"45\"),\n",
    "                   (\"Category:Article_45(4)_GDPR\",\"45\"),\n",
    "                   (\"Category:Article_45(5)_GDPR\",\"45\"),\n",
    "                   (\"Category:Article_45(6)_GDPR\",\"45\"),\n",
    "                   (\"Category:Article_45(7)_GDPR\",\"45\"),\n",
    "                   (\"Category:Article_45(8)_GDPR\",\"45\"),\n",
    "                   (\"Category:Article_45(9)_GDPR\",\"45\"),\n",
    "                   (\"Category:Article_46_GDPR\",\"46\"),\n",
    "                   (\"Category:Article_46(1)_GDPR\",\"46\"),\n",
    "                   (\"Category:Article_46(2)_GDPR\",\"46\"),\n",
    "                   (\"Category:Article_46(3)_GDPR\",\"46\"),\n",
    "                   (\"Category:Article_46(4)_GDPR\",\"46\"),\n",
    "                   (\"Category:Article_46(5)_GDPR\",\"46\"),\n",
    "                  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text(page_id):\n",
    "    response = requests.get(API_ENPOINT + f\"?action=query&pageids={page_id}&format=json&prop=revisions&rvslots=*&rvprop=content&formatversion=2\")\n",
    "    json_data = response.json()\n",
    "    return re.sub(r\"\\\\n\", \"\\n\", json_data['query']['pages'][0]['revisions'][0]['slots']['main']['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The API paginates the result, so we have to walk through all pages to get all relevant decisions. \n",
    "\n",
    "Please not that some of the decisions are duplicated, as they might belong to multiple categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Category:Article_44_GDPR 44\n",
      "Processing Category:Article_44_GDPR 44\n",
      "Processing Category:Article_44_GDPR 44\n",
      "Processing Category:Article_44_GDPR 44\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 31/31 [00:15<00:00,  2.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Category:Article_45_GDPR 45\n",
      "Processing Category:Article_45_GDPR 45\n",
      "Processing Category:Article_45_GDPR 45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 23/23 [00:07<00:00,  3.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Category:Article_45(1)_GDPR 45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Category:Article_45(2)_GDPR 45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 64860.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Category:Article_45(3)_GDPR 45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00,  2.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Category:Article_45(4)_GDPR 45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Category:Article_45(5)_GDPR 45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Category:Article_45(6)_GDPR 45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Category:Article_45(7)_GDPR 45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Category:Article_45(8)_GDPR 45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Category:Article_45(9)_GDPR 45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Category:Article_46_GDPR 46\n",
      "Processing Category:Article_46_GDPR 46\n",
      "Processing Category:Article_46_GDPR 46\n",
      "Processing Category:Article_46_GDPR 46\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 34/34 [00:13<00:00,  2.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Category:Article_46(1)_GDPR 46\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:02<00:00,  1.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Category:Article_46(2)_GDPR 46\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:00<00:00, 13.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Category:Article_46(3)_GDPR 46\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 22982.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Category:Article_46(4)_GDPR 46\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Category:Article_46(5)_GDPR 46\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "for category_title, category_id in category_titles:\n",
    "    items = []\n",
    "    continue_query = \"\"\n",
    "    while(True):\n",
    "        print(f\"Processing {category_title} {category_id}\")\n",
    "        response = requests.get(API_ENPOINT + f\"?action=query&prop=categories&format=json&list=categorymembers&cmtitle={category_title}{continue_query}\")\n",
    "        json_data = response.json()\n",
    "        for item in json_data['query']['categorymembers']:\n",
    "            items.append(item)\n",
    "\n",
    "        if('continue' not in json_data):\n",
    "            break\n",
    "\n",
    "        continue_id = json_data['continue']['cmcontinue']\n",
    "        continue_query = f\"&cmcontinue={continue_id}\"\n",
    "    for item in tqdm.tqdm(items):\n",
    "        if(item['ns'] == 0):\n",
    "            # regular page\n",
    "            id = item['pageid']\n",
    "            directory = f\"data/gdprhub/art-{category_id}/\"\n",
    "            if(not os.path.exists(directory)):\n",
    "                os.mkdir(directory)\n",
    "            with open(directory + f\"{id}.txt\", \"w\") as output:\n",
    "                output.write(get_text(id))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract content of decisions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_structure(text):\n",
    "    section = []\n",
    "    structure = {\"preamble\": section}\n",
    "    \n",
    "    for line in text:\n",
    "        if(re.match(r\"^={1,3}[^=]\", line)):\n",
    "            match = re.match(r\"^={1,3}([^=]+)={1,3}\", line)\n",
    "            section_name = match[1].strip()\n",
    "            section = []\n",
    "            structure[section_name] = section\n",
    "        else:\n",
    "            section.append(line)\n",
    "\n",
    "    return structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_parts(text):\n",
    "    lines = text.split(\"\\n\")\n",
    "\n",
    "    infobox = []\n",
    "    description = []\n",
    "    translation = []\n",
    "    # 0 init state\n",
    "    # 1 bbox\n",
    "    # 2 description\n",
    "    # 3 translation\n",
    "    state = 0 \n",
    "\n",
    "    for line in lines:\n",
    "        if(len(line) == 0):\n",
    "            continue\n",
    "\n",
    "        if(re.match(r\"^{{\", line)):\n",
    "            state = 1\n",
    "        elif(re.match(r\"^[|]?}}\", line)):\n",
    "            state = 2\n",
    "            continue\n",
    "        elif(re.match(r\"^<pre>\", line)):\n",
    "            state = 3\n",
    "            continue\n",
    "        elif(re.match(r\"<\\/pre>\", line)):\n",
    "            state = 0\n",
    "\n",
    "        if(state == 1):\n",
    "            infobox.append(line)\n",
    "        elif(state == 2):\n",
    "            description.append(line)\n",
    "        elif(state == 3):\n",
    "            translation.append(line)\n",
    "\n",
    "\n",
    "    return {\"infobox\": infobox, \"description\": extract_structure(description), \"translation\": translation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentences(path, processor, key_path):\n",
    "    text = \"\"\n",
    "    with open(path) as input:\n",
    "        text = input.read()\n",
    "    parts = extract_parts(text)\n",
    "    \n",
    "    item = parts\n",
    "    for key in key_path:\n",
    "        try:\n",
    "            item = item[key]\n",
    "        except KeyError:\n",
    "            return []\n",
    "    return processor.extract_sentences(item).sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import stanza \n",
    "\n",
    "class StanzaProcessor:\n",
    "    def __init__(self):\n",
    "        self.pipeline = stanza.Pipeline(lang='en', processors='tokenize')\n",
    "\n",
    "    def extract_sentences(self, text):\n",
    "        return self.pipeline(\" \".join(text))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract individual sentences from the GDPRHub decisions. We take into account only `Holding` and `Facts`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-28 12:55:59 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3915513fa52e494697e5f44b34336086",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.5.0.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-28 12:55:59 INFO: Loading these models for language: en (English):\n",
      "========================\n",
      "| Processor | Package  |\n",
      "------------------------\n",
      "| tokenize  | combined |\n",
      "========================\n",
      "\n",
      "2023-09-28 12:55:59 INFO: Using device: cpu\n",
      "2023-09-28 12:55:59 INFO: Loading: tokenize\n",
      "2023-09-28 12:55:59 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "\n",
    "files = {}\n",
    "file_names = set()\n",
    "processor = StanzaProcessor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/gdprhub/art-44/4253.txt 33\n",
      "data/gdprhub/art-44/5996.txt 24\n",
      "data/gdprhub/art-44/6091.txt 20\n",
      "data/gdprhub/art-44/4186.txt 10\n",
      "data/gdprhub/art-44/6174.txt 26\n",
      "data/gdprhub/art-44/5093.txt 44\n",
      "data/gdprhub/art-44/3241.txt 29\n",
      "data/gdprhub/art-44/6092.txt 18\n",
      "data/gdprhub/art-44/3408.txt 14\n",
      "data/gdprhub/art-44/5938.txt 18\n",
      "data/gdprhub/art-44/3233.txt 16\n",
      "data/gdprhub/art-44/5399.txt 18\n",
      "data/gdprhub/art-44/5914.txt 34\n",
      "data/gdprhub/art-44/5359.txt 64\n",
      "data/gdprhub/art-44/3423.txt 26\n",
      "data/gdprhub/art-44/6198.txt 19\n",
      "data/gdprhub/art-44/4486.txt 41\n",
      "data/gdprhub/art-44/4122.txt 20\n",
      "data/gdprhub/art-44/5110.txt 30\n",
      "data/gdprhub/art-44/6087.txt 22\n",
      "data/gdprhub/art-44/2804.txt 44\n",
      "data/gdprhub/art-44/4628.txt 32\n",
      "data/gdprhub/art-44/5716.txt 49\n",
      "data/gdprhub/art-44/3180.txt 27\n",
      "data/gdprhub/art-44/3240.txt 7\n",
      "data/gdprhub/art-44/5526.txt 36\n",
      "data/gdprhub/art-44/6090.txt 20\n",
      "data/gdprhub/art-44/5627.txt 28\n",
      "data/gdprhub/art-44/5743.txt 52\n",
      "data/gdprhub/art-44/5028.txt 25\n",
      "data/gdprhub/art-44/5953.txt 12\n",
      "data/gdprhub/art-45/5624.txt 27\n",
      "data/gdprhub/art-45/3900.txt 4\n",
      "data/gdprhub/art-45/5944.txt 29\n",
      "data/gdprhub/art-45/5288.txt 17\n",
      "data/gdprhub/art-45/3659.txt 48\n",
      "data/gdprhub/art-45/5101.txt 8\n",
      "data/gdprhub/art-45/2875.txt 43\n",
      "data/gdprhub/art-45/2575.txt 5\n",
      "data/gdprhub/art-46/4976.txt 5\n",
      "data/gdprhub/art-46/5506.txt 29\n",
      "data/gdprhub/art-46/5956.txt 42\n",
      "data/gdprhub/art-46/5947.txt 6\n",
      "data/gdprhub/art-46/5310.txt 24\n",
      "data/gdprhub/art-46/4353.txt 13\n",
      "data/gdprhub/art-46/5396.txt 15\n",
      "data/gdprhub/art-46/1515.txt 8\n",
      "data/gdprhub/art-46/4622.txt 39\n",
      "data/gdprhub/art-46/6188.txt 19\n",
      "1239\n",
      "1178\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for category_id in [\"44\", \"45\", \"46\"]:\n",
    "    for idx, fname in enumerate(glob.glob(f\"data/gdprhub/art-{category_id}/*.txt\")):\n",
    "        file_name = fname.split(\"/\")[-1]\n",
    "        if file_name in file_names:\n",
    "            continue\n",
    "        file_names.add(file_name)\n",
    "        files[fname] = []\n",
    "        files[fname][0:0] = list(get_sentences(fname, processor, ['description', 'Holding']))\n",
    "        files[fname][0:0] = list(get_sentences(fname, processor, ['description', 'Facts']))\n",
    "\n",
    "sentence_objects = {}\n",
    "sentences = []\n",
    "for idx,fname in enumerate(files):\n",
    "    print(fname, len(files[fname]))\n",
    "    sentences[0:0] = [s.text for s in files[fname]]\n",
    "    sentence_objects.update({(s.text, (s,fname,i)) for i,s in enumerate(files[fname]) if s.text not in sentence_objects})\n",
    "    \n",
    "    \n",
    "print(len(sentences))\n",
    "print(len(sentence_objects))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stage 2 - find sentences that match the questions (slow!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load questions generated by ChatGPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of questions: 55\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "path = \"data/\"\n",
    "\n",
    "items = []\n",
    "\n",
    "with open(f\"{path}questions.jsonl\") as input:\n",
    "    for idx, line in enumerate(input):\n",
    "        if(len(line.strip()) == 0):\n",
    "            continue\n",
    "        items.append(json.loads(line))\n",
    "\n",
    "print(f\"Number of questions: {len(items)}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download and load the model trained in the experiment. This is AlBERT-xxl-v1 trained on SQuAD 2.0 sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d90d059245b445ab5cb0a994fca7534",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/914 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b028c93145d34dd9ae12cdf34b205507",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/890M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ba10df2139d4481b6096344d5a5b3c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading spiece.model:   0%|          | 0.00/760k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2424df2fda1b4cfd8c8ead46743ea187",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/1.31M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"apohllo/albert-xxl-squad-sentences\", num_labels=2)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"apohllo/albert-xxl-squad-sentences\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Add device=0 if you want to use GPU!\n",
    "classifier = pipeline(\"text-classification\", model=model, tokenizer=tokenizer, batch_size=16) #, device=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_results(question, sentences, classifier, top_k=5):\n",
    "    samples = [{\"text\": s, \"text_pair\": question} for s in sentences]\n",
    "    results = classifier(samples)\n",
    "    \n",
    "    results = [(idx, r[\"score\"]) if r[\"label\"] == 'LABEL_1' else (idx, 1 - r[\"score\"]) \n",
    "            for idx, r in enumerate(results)]\n",
    "    \n",
    "    keys_values = sorted(results, key=lambda e: -e[1])[:top_k]\n",
    "    return [(v,sentences[k]) for k,v in keys_values]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On CPU this will run for hours! Albert-XXL is a pretty large model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"{path}/sentences.jsonl\", \"w\") as output:\n",
    "    for item in tqdm.tqdm(items):\n",
    "        results = top_results(item[\"question\"], sentences, classifier, top_k=10)\n",
    "        results = [{\"score\":v, \"sentence\":s} for v,s in results]\n",
    "        item[\"sentences\"] = results\n",
    "        output.write(json.dumps(item) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stage 3 - answer the questions using Flan-T5-large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c05629607eaa458988c05f8c3bf9d8d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/662 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc43cfc5ed0d4db3aab976f665dc33db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading model.safetensors:   0%|          | 0.00/3.13G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2a649076b994edf9e351b9652792e53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)neration_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9cec4fd0d7c341ceadae207405014175",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/2.54k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0177fbfb4fdb437ab494b7893288467b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8e9d4b12bc24efdbcc43578d8150eec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/2.42M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98456d2e5c204bf2a435aa681a80c7c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/2.20k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "model_name = \"google/flan-t5-large\"\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "generator = pipeline(\"text2text-generation\", model=model, tokenizer=tokenizer, batch_size=16) #, device=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "with open(\"data/sentences.jsonl\") as input:\n",
    "    for line in input:\n",
    "        data.append(json.loads(line.strip()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_length = 1\n",
    "\n",
    "# We have selected some sentences below the threshold to see how the model works for them\n",
    "additional_sentences = set([(16,0), (25,0), (39,0), (41,0)])\n",
    "\n",
    "# The threshold was selected to get equal error rate\n",
    "threshold = 0.65\n",
    "\n",
    "with open(\"data/answers.jsonl\", \"w\") as json_output:\n",
    "    for i_idx, item in tqdm.tqdm(enumerate(data)):\n",
    "        for s_idx, sentence in enumerate(item['sentences']):\n",
    "            if sentence['score'] > threshold or (i_idx + 1, s_idx) in additional_sentences:\n",
    "                print(i_idx, s_idx, \"%.3f\" % sentence['score'])\n",
    "                sentence_object, fname, sentence_index = sentence_objects[sentence['sentence']]\n",
    "                context = []\n",
    "                print(fname, sentence_index)\n",
    "                if sentence_index - context_length >= 0:\n",
    "                    for i in range(context_length):\n",
    "                        context.append(files[fname][sentence_index - context_length + i])\n",
    "                context.append(sentence_object)\n",
    "                if sentence_index + context_length < len(files[fname]):\n",
    "                    for i in range(context_length):\n",
    "                        context.append(files[fname][sentence_index + i + 1])\n",
    "\n",
    "                context_text = \"\"\n",
    "                for idx,sentence in enumerate(context):\n",
    "                    context_text += sentence.text + \" \"\n",
    "\n",
    "                tuple = {}\n",
    "                tuple[\"concept\"] = item['concept']\n",
    "                tuple[\"question\"] = item['question']\n",
    "                tuple[\"context\"] = context_text\n",
    "                prompt = f\"Given the information: \\\"{context_text}\\\" answer the following question: {item['question']}\"\n",
    "                answer = generator(prompt)[0]['generated_text']\n",
    "                tuple[\"answer\"] = answer\n",
    "                json_output.write(json.dumps(tuple) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stage 4 - summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "concepts = []\n",
    "with open(\"data/answers.jsonl\") as input:\n",
    "    for line in input:\n",
    "        data = json.loads(line)\n",
    "        concept = data[\"concept\"]\n",
    "        if(len(concepts) > 0 and concepts[-1][\"concept\"] == concept):\n",
    "            concepts[-1][\"examples\"].append({\"example\": data[\"context\"], \"answer\": data[\"answer\"]})\n",
    "        else:\n",
    "            concepts.append({\"concept\": concept, \"examples\": [{\"example\": data[\"context\"], \"answer\": data[\"answer\"]}]})\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================\n",
      "** Enforceable data subject rights **\n",
      "\n",
      "It also appeared that the company's files contained several excessive comments\n",
      "related to customers or their health conditions. In addition, people were not\n",
      "properly informed about the processing of their personal data, or about the\n",
      "recording of the conversations they had with the company. In total, following\n",
      "its investigations the CNIL found five breaches of the GDPR: -         Violation\n",
      "of the right to object, Article 21(2) GDPR: no procedure was implemented to\n",
      "ensure effectively that persons who opposed telephone solicitation were no\n",
      "longer called); -         Violation of the principle of data minimization,\n",
      "Article 5(1)(c) GDPR: inadequate and offensive comments or irrelevant comments\n",
      "related to people's health were found in the company's customer file; -        \n",
      "Violation of Articles 12 and 13 GDPR: insufficient information on the processing\n",
      "of data subject’s personal data and their rights; -         Violation of\n",
      "Articles 46 and 49 GDPR:  the controller did not provide appropriate safeguards\n",
      "for data subjects; -         Failure to cooperate with the CNIL, Article 31\n",
      "GDPR.\n",
      "\n",
      "It was therefore necessary for the international agreement, as the legal basis\n",
      "for the transfer, to include appropriate data protection safeguards under\n",
      "[[Article 46 GDPR|Article 46(2)(a) GDPR]]. In this case, the DPA found that the\n",
      "agreement contained no definition of data protection, no retention period, no\n",
      "mention of the rights of data subjects and no mention of appeal mechanisms. The\n",
      "DPA therefore concluded that the Belgian tax authority could not rely on\n",
      "[[Article 96 GDPR|Article 96 GDPR]] to continue transferring data to the US on\n",
      "the basis of the FATCA agreement when that agreement is not in line with the\n",
      "GDPR.\n",
      "\n",
      "The HDPA identified deficiencies as follows: first of all, the HDPA found that\n",
      "the Ministry never made a detailed investigation on the lawfulness of the\n",
      "processing purposes under [[Article 6 GDPR#4|Article 6(4) GDPR]], in particular\n",
      "with regard to the consent for access to information stored in a user's terminal\n",
      "equipment, when is not necessary to provide the service requested by the user.\n",
      "Regarding the principle of transparency and the right to access by the data\n",
      "subject, according to Article 12 and 14 GDPR, the information provided by the\n",
      "Ministry to the data subjects was not considered appropriate and sufficient. The\n",
      "HDPA found in particular that the provided information was not easy to\n",
      "understand and (lack of accessibility and of clear and simple wording),\n",
      "especially vis-à-vis children.\n",
      "\n",
      "In addition, people were not properly informed about the processing of their\n",
      "personal data, or about the recording of the conversations they had with the\n",
      "company. In total, following its investigations the CNIL found five breaches of\n",
      "the GDPR: -         Violation of the right to object, Article 21(2) GDPR: no\n",
      "procedure was implemented to ensure effectively that persons who opposed\n",
      "telephone solicitation were no longer called); -         Violation of the\n",
      "principle of data minimization, Article 5(1)(c) GDPR: inadequate and offensive\n",
      "comments or irrelevant comments related to people's health were found in the\n",
      "company's customer file; -         Violation of Articles 12 and 13 GDPR:\n",
      "insufficient information on the processing of data subject’s personal data and\n",
      "their rights; -         Violation of Articles 46 and 49 GDPR:  the controller\n",
      "did not provide appropriate safeguards for data subjects; -         Failure to\n",
      "cooperate with the CNIL, Article 31 GDPR. As a consequence, the CNIL imposed a\n",
      "fine of EUR 500.000.\n",
      "\n",
      "The AEPD also evaluated the position of the three entities involved in the case,\n",
      "and held that according to the processing agreements that were in place, Amazon\n",
      "Road was the controller responsible for the processing carried out by Amazon\n",
      "India and Accurate Background, and that because these processors were located\n",
      "outside the EEA (in India and the United States respectively), international\n",
      "data transfers were taking place. The AEPD found that, in this case, data\n",
      "subjects' consent to data transfers would not be valid in accordance with\n",
      "[[Article 49 GDPR|Article 49(1) GDPR]]  and [[Article 7 GDPR]], given that\n",
      "consent was required within the contract without an option to refuse, it was not\n",
      "explicit, and no information was given to the data subject regarding the risks\n",
      "of these data transfers. However, the AEPD found that the data transfers were\n",
      "lawful according to [[Article 46 GDPR]], since the SCCs in Amazon's processing\n",
      "agreements included appropriate technical and organisational data protection\n",
      "measures, and Accurate Background was adhered to the EU-US Privacy Shield during\n",
      "the time that the data transfers took place.\n",
      "\n",
      "These law allow for bulk collection of personal data. They do not allow a data\n",
      "subject to enforce any rights before a tribunal. ''With regard to national law\n",
      "relating to collection and processing of data:''   The French Court outlined\n",
      "that Article L. 1462-1 of the public health code provides for the Health Data\n",
      "Hub and the collection of health data from the existing national health data\n",
      "system (as per Article L. 1461-1).\n",
      "\n",
      "The DPA remarks that, according the Schrems II Judgment, the transfer of data to\n",
      "the United States may result in violations of fundamental rights, given that the\n",
      "US legislation allows for access to the data because of national security and\n",
      "public interest reasons. Such inferences are not reasonable, as limitations to\n",
      "fundamental rights are not clearly defined; as there are no clear and precise\n",
      "rules on the application of such measures or minimum requirements to protect\n",
      "against risks of abuse; there is no requirement for a necessity test; and there\n",
      "are no enforceable rights for data subjects or legal remedies. The Portuguese\n",
      "DPA found that the National Institute had not undertaken a sufficient Data\n",
      "Protection Impact Assessment, had not consulted the supervisory authority prior\n",
      "to processing, and had therefore not adopted adequate additional safeguards\n",
      "before using the services of a data processor who was headquartered in the\n",
      "United States.\n",
      "==============================\n",
      "** Effective legal remedies for data subjects **\n",
      "\n",
      "The parties still agreed supplementary measures. First, Google has established\n",
      "policies and procedures and a team of qualified lawyers for handling authority\n",
      "requests for user data. Second, Google offers and the controller implemented the\n",
      "Anonymize IP feature and, finally, the controller had applied a redaction script\n",
      "on the website to prevent personal data unintentionally being shared with\n",
      "Google.\n",
      "==============================\n",
      "** Legally binding and enforceable instrument **\n",
      "\n",
      "In its privacy policy the controller simply informed consumers about data\n",
      "transfers to third parties and countries, without any further legal effect. This\n",
      "document did not constitute a legally binding contract offered to customers by\n",
      "the controller, as the Consumer Center suggested. The court also held that the\n",
      "Consumer Center's claim with regard to the cookie banners was unfounded.\n",
      "\n",
      "Under national US law, Google LLC, as a provider of electronic communication\n",
      "services is subject to surveillance by the intelligence agencies and is thus\n",
      "obliged to provide the US government with personal data. According to the\n",
      "Schrems judgment, that the DPA considered up-to-date, this legislation doesn’t\n",
      "meet the requirements of EU law. Fourthly, considering that the SCC’s were not\n",
      "sufficient, the DPA assessed whether the controller and the processor\n",
      "implemented additional safeguards for the data transfers.\n",
      "\n",
      "Under national US law, Google LLC, as a provider of electronic communication\n",
      "services is subject to surveillance by the intelligence agencies and is thus\n",
      "obliged to provide the US government with personal data. According to the\n",
      "Schrems judgment, that the DPA considered up-to-date, this legislation doesn’t\n",
      "meet the requirements of EU law. Fourthly, considering that the SCC’s were not\n",
      "sufficient, the DPA assessed whether the controller and the processor\n",
      "implemented additional safeguards for the data transfers.\n",
      "\n",
      "Under national US law, Google LLC, as a provider of electronic communication\n",
      "services is subject to surveillance by the intelligence agencies and is thus\n",
      "obliged to provide the US government with personal data. According to the\n",
      "Schrems judgment, that the DPA considered up-to-date, this legislation doesn’t\n",
      "meet the requirements of EU law. Fourthly, considering that the SCC’s were not\n",
      "sufficient, the DPA assessed whether the controller and the processor\n",
      "implemented additional safeguards for the data transfers.\n",
      "==============================\n",
      "** Binding corporate rules **\n",
      "\n",
      "The parties still agreed supplementary measures. First, Google has established\n",
      "policies and procedures and a team of qualified lawyers for handling authority\n",
      "requests for user data. Second, Google offers and the controller implemented the\n",
      "Anonymize IP feature and, finally, the controller had applied a redaction script\n",
      "on the website to prevent personal data unintentionally being shared with\n",
      "Google.\n",
      "\n",
      "First, the Irish DPA ascertained whether US law guaranteed an essentially\n",
      "equivalent level of protection of data protection rights in light of Schrems II.\n",
      "This was excluded by the supervisory authority, especially due to the lack of\n",
      "effective judicial remedies against the violation of data subjects’ fundamental\n",
      "rights by U.S. intelligence agencies and due to the lack of limitations imposed\n",
      "on the latters’ investigation powers. The latest developments in U.S. law (which\n",
      "are supposed to ensure a higher level of protection for data transferred to the\n",
      "U.S.) were deemed insufficient by the Irish DPA, especially since some of the\n",
      "promised reforms have not yet been implemented.\n",
      "==============================\n",
      "** Standard data protection clauses **\n",
      "\n",
      "The Norwegian DPA then proceeded to investigate the case and notified the\n",
      "controller asking for information about the investigated facts. In response, the\n",
      "controller stated that after becoming aware of the judgment in the Schrems II\n",
      "case, it reassessed its contract with Google, the processor, and adopted\n",
      "standard contractual clauses (SCCs) as the legal basis for the transfer of data\n",
      "to the US. Furthermore, it claimed that it was taking additional measures to\n",
      "ensure the protection of personal data transferred outside the EU/EEA.\n",
      "\n",
      "According to the website provider and Google LLC, the website controller\n",
      "qualifies as controller ([[Article 4 GDPR#7|Article 4(7) GDPR]]) and Google LLC\n",
      "as processor ([[Article 4 GDPR#8|Article 4(8) GDPR]]) for data processing in\n",
      "connection with Google Analytics. Furthermore, according to the privacy\n",
      "documents provided on the website or included via hyperlink, the website\n",
      "provider and Google LLC entered into standard contractual clauses under\n",
      "[[Article 46 GDPR#2#c|Article 46(2)(c) GDPR]] ([https://eur-lex.europa.eu/legal-\n",
      "content/EN/ALL/?uri=celex%3A32010D0087 Commission Decision2010/87 of\n",
      "05.02.2010]; SCCs) as a mechanism for transfers of personal data with regard to\n",
      "Google Analytics. On 18.08.2020, the data subject (represented by ''noyb'')\n",
      "filed a complaint with the DSB against both the website provider (in its role as\n",
      "data exporter) and Google LLC (in its role as data importer), arguing that both\n",
      "respondents violated Articles 44 et. seqq.\n",
      "\n",
      "They found this to apply to their use of Google Analytics since the agreement\n",
      "was with Google as a US processor of their. With the Privacy Shield now\n",
      "invalidated, the controller entered into standard contractual clauses (SCCs)\n",
      "Module Two with Google on 12 August 2020 for data transfers to the US. However,\n",
      "the controller did not carry out a thorough review of potential third country\n",
      "legislation (a \"transfer impact assessment\"), as it, according to information\n",
      "from Google, was not possible to determine the exact location of processing.\n",
      "\n",
      "This general consent clause also stated that it would exonerate Amazon of any\n",
      "responsibility, damages claims, or other charges related to the processing and\n",
      "transfer of data as far as the law permits it. Amazon Road established an Intra-\n",
      "Group Data Transfer and Processing Agreement with Amazon India and a Data\n",
      "Processing Agreement with Accurate Background, which both included Standard\n",
      "Contractual Clauses (SCCs) with technical and organisational measures required\n",
      "for data processing. Additionally, Accurate Background was adhered to the EU-US\n",
      "Privacy Shield transatlantic data transfer framework.\n",
      "\n",
      "For the use of this tool, the controller transferred users’ personal data to the\n",
      "processor, in the US. The controller and Google had implemented standard\n",
      "contractual clauses (‘SCCs’) within the meaning of [[Article 46 GDPR]]. In 2020,\n",
      "''noyb'' lodged a complaint with the Austrian DPA alleging that the controller\n",
      "breached the provisions of Chapter V GDPR.\n",
      "\n",
      "Meta Ireland had been transferring personal data to the U.S. despite the lack of\n",
      "a valid adequacy decision under [[Article 45 GDPR]] (as both “safe harbor” and\n",
      "its successor “privacy shield” were invalidated by the CJEU in Schrems I and\n",
      "II). While negotiation of a new adequacy decision for EU-U.S. data transfers are\n",
      "ongoing, Meta Ireland claimed to have undertaken data transfers on the basis of\n",
      "standard contractual clauses adopted by the Commission under [[Article 46\n",
      "GDPR#2c|Article 46(2)(c) GDPR]] even before the CJEU passed the Schrems II\n",
      "decision. First, the Irish DPA ascertained whether US law guaranteed an\n",
      "essentially equivalent level of protection of data protection rights in light of\n",
      "Schrems II.\n",
      "\n",
      "After establishing this, the Court emphasized that the validity of the SCCs,\n",
      "however, did depend on whether there were effective mechanisms in place that\n",
      "make it possible to ensure compliance with the level of protection required by\n",
      "EU law. Important to note is that here the Court held that the SCCs in\n",
      "themselves did provide for such mechanisms. However, it went on to stress that\n",
      "where these mechanisms cannot be complied with, the transfers of personal data\n",
      "pursuant to these clauses is to be suspended or prohibited.\n",
      "==============================\n",
      "** Competent supervisory authority **\n",
      "\n",
      "In other words, the fact that the controller claimed to have no knowledge\n",
      "whether data were transferred to the US by Meta Ireland showed that the\n",
      "controller disregarded its responsibilities under the GDPR. In light of the\n",
      "above, the Danish DPA reprimanded the controller and ordered it to bring its\n",
      "processing activities in compliance with [[Article 25 GDPR|Article 25]],\n",
      "[[Article 5 GDPR|5]] and [[Article 24 GDPR|24 GDPR]]. within a month.\n",
      "==============================\n",
      "** Contractual clauses **\n",
      "\n",
      "Its servers were located in the EU. Company C included clauses in its offer\n",
      "stating that it would not disclose customer data to any third party, except as\n",
      "necessary to maintain or provide the services, or as necessary to comply with\n",
      "the law or a valid and binding order of a governmental body. After reviewing the\n",
      "offers, the publicly owned company issued a decision where it awarded the\n",
      "contract to Company A, as their evaluation of the price was the most economical.\n",
      "\n",
      "Examining the decision in light of the provisions of the Charter, the Court held\n",
      "that the requirements of US national security, public interest, and law\n",
      "enforcement do in fact interfere with the fundamental rights of persons whose\n",
      "data is transferred there. These limitations on the protection of personal data\n",
      "were not circumscribed in a way that satisfied requirements that are essentially\n",
      "equivalent to those required under EU law. The principle of proportionality was\n",
      "also not satisfied, in so far as US surveillance programs are not limited to\n",
      "what is ‘strictly necessary’.\n",
      "\n",
      "The Municipality, in its capacity as the controller, instructed its processor\n",
      "(Google Ireland) to transfer personal data to a sub-processor (Google LLC) in\n",
      "the United States. The transfer was based on standard data protection clauses\n",
      "pursuant to [[Article 46 GDPR|Article 46(2)(c) GDPR]]. In C-311/18, Schrems II,\n",
      "the CJEU clarified that the use of SCCs does not always constitute \"an adequate\n",
      "means of ensuring the effective protection of the personal data transferred to\n",
      "the third country in question in practice.\n",
      "\n",
      "For the use of this tool, the controller transferred users’ personal data to the\n",
      "processor, in the US. The controller and Google had implemented standard\n",
      "contractual clauses (‘SCCs’) within the meaning of [[Article 46 GDPR]]. In 2020,\n",
      "''noyb'' lodged a complaint with the Austrian DPA alleging that the controller\n",
      "breached the provisions of Chapter V GDPR.\n",
      "\n",
      "It considered whether the respondent could rely on any transfer mechanisms under\n",
      "Chapter V. of the GDPR and held:  * The respondent could not rely on an adequacy\n",
      "decision following [[CJEU - C-311/18 - Schrems II|C-311/18]]. * The SCCs\n",
      "concluded between the respondent and Google LLC do not offer an adequate level\n",
      "of protection, because:  ** Google LLC qualifies as an \"''electronic\n",
      "communication service provider''\" under 50 U.S. Code § 1881(b)(4) and is subject\n",
      "to surveillance by US intelligence services, and ** any contractual,\n",
      "organisational and technical measures which Google put into place to complement\n",
      "the SCCs were insufficient as they could not prevent US intelligence services\n",
      "from accessing the data subject's personal data ** Notably, the CNIL rejected\n",
      "Google's argument that any Google Analytics data were pseudonymised,\n",
      "highlighting that Universal Unique Identifiers do not meet the definition of\n",
      "pseudonymisation under [[Article 4 GDPR#5|Article 4(5) GDPR]], as their sole\n",
      "purpose is to identify users. *\n",
      "==============================\n",
      "** Administrative arrangements **\n",
      "\n",
      "Examining the decision in light of the provisions of the Charter, the Court held\n",
      "that the requirements of US national security, public interest, and law\n",
      "enforcement do in fact interfere with the fundamental rights of persons whose\n",
      "data is transferred there. These limitations on the protection of personal data\n",
      "were not circumscribed in a way that satisfied requirements that are essentially\n",
      "equivalent to those required under EU law. The principle of proportionality was\n",
      "also not satisfied, in so far as US surveillance programs are not limited to\n",
      "what is ‘strictly necessary’.\n",
      "\n",
      "The DPA remarks that, according the Schrems II Judgment, the transfer of data to\n",
      "the United States may result in violations of fundamental rights, given that the\n",
      "US legislation allows for access to the data because of national security and\n",
      "public interest reasons. Such inferences are not reasonable, as limitations to\n",
      "fundamental rights are not clearly defined; as there are no clear and precise\n",
      "rules on the application of such measures or minimum requirements to protect\n",
      "against risks of abuse; there is no requirement for a necessity test; and there\n",
      "are no enforceable rights for data subjects or legal remedies. The Portuguese\n",
      "DPA found that the National Institute had not undertaken a sufficient Data\n",
      "Protection Impact Assessment, had not consulted the supervisory authority prior\n",
      "to processing, and had therefore not adopted adequate additional safeguards\n",
      "before using the services of a data processor who was headquartered in the\n",
      "United States.\n",
      "\n",
      "These law allow for bulk collection of personal data. They do not allow a data\n",
      "subject to enforce any rights before a tribunal. ''With regard to national law\n",
      "relating to collection and processing of data:''   The French Court outlined\n",
      "that Article L. 1462-1 of the public health code provides for the Health Data\n",
      "Hub and the collection of health data from the existing national health data\n",
      "system (as per Article L. 1461-1).\n",
      "\n",
      "The principle of proportionality was also not satisfied, in so far as US\n",
      "surveillance programs are not limited to what is ‘strictly necessary’. It was\n",
      "noted that the provisions in the US surveillance programs neither limited the\n",
      "power they conferred onto national authorities, nor granted data subjects\n",
      "actionable rights before the courts against the US authorities. The Court\n",
      "proceeded to scrutinize the Ombudsperson mechanism that had been in place under\n",
      "the Privacy Shield, stating that it too did not provide data subjects with a\n",
      "cause of action before a body which was fully independent, and that this body\n",
      "was limited in so far as it could not impose rules that were binding on US\n",
      "intelligence services.\n",
      "==============================\n",
      "** Consistency mechanism **\n",
      "\n",
      "The DPA concluded that Portuguese citizens lack any guarantees in regards to\n",
      "their data being collected by the National Statistical Institute, as US\n",
      "legislation does not offer a similar level of protection than the GDPR. The\n",
      "controller had neither been able to demonstrate that the data is not effectively\n",
      "transferred to the US, not had they implemented any supplementary adequate\n",
      "measures to ensure a similar level of protection, which they are obliged to do\n",
      "as a data controller. Therefore, the CNPD ordered the National Statistical\n",
      "Institute to suspend any processing of personal data for the census in the US or\n",
      "any other third country without adequate levels of protection, within 12 hours\n",
      "of their decision being issued.\n"
     ]
    }
   ],
   "source": [
    "import textwrap\n",
    "\n",
    "for item in concepts:\n",
    "    print(\"=\" * 30)\n",
    "    print(\"** \" + item[\"concept\"] + \" **\")\n",
    "    for example in item[\"examples\"]:\n",
    "        print(\"\")\n",
    "        print(textwrap.fill(example['example'], 80))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "exaile",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
