Our methodology for finding the relevant examples can be divided into the following steps:
\begin{enumerate}
  \item Given a legal concept, generate a number of questions that ask if the concept was applicable in a given context. The precise number and form of the questions is variable.
  \item Once a question is generated, we use a binary classification model to find sentences in court cases which answer the question. 
    We use the predicted probability not only to sort these sentences, but also to filter sentences below a defined threshold.
    As a result this step might generate an empty set, meaning that in our corpus there are no relevant sentences. 
    This prevents the system from presenting passages that are irrelevant, but also limits the possibility of false
    classification of the sentences.
  \item In the last step, we extend the sentence to include the preceding and the following sentences forming a three-sentence passage
    and ask an instruction-following model to judge if the example gives a positive or a negative answer to the generated question.
    We use that prediction to classify the passage as a positive or a negative example.
\end{enumerate}

It has to be stressed that none of the steps uses a domain-specific (legal) question-answering dataset. We only use an open-domain dataset (SQuAD) and general generative models (ChatGPT and Flan-T5). 
This means that the presented method works without a dedicated legal dataset, in contrast to most other neural systems related to legal question answering. We believe that using such a dedicated dataset will improve the obtained results, but this has to be confirmed in the future work.

%We follow with a detailed description of the procedure.

\subsection{Question generation}

In order to generate questions, we first have to choose relevant concepts. This raises an interesting connection with hybrid AI solutions, as the concepts could be extracted from knowledge representation databases such as legal ontologies. For the purpose of this paper, the authors have read the relevant legal articles and have chosen 11 concepts according to their understanding.

In order to generate the questions from the concepts, we have approached ChatGPT\footnote{June 22 Version using GPT-4} with the following prompt "For each of the following legal concepts from the GDPR, please give 4 specific and 1 general yes/no questions, which might be discussed by a court". The specific number and type of questions to generate is variable and depend on the legislation.

Since we focus on the GDPR, we assume that ChatGPT has a good knowledge of the relevant legislation. As the results show (cf. Table \ref{tab:question-examples}), this was a sensible assumption. However, for a solution that is independent from the knowledge of the language model (i.e. taking into account some amendments that were applied recently), we could provide the
relevant provision as a part of the prompt or via further fine-tuning.

\begin{table}[htbp]
  \centering\begin{tabular}{l p{6cm}}
    \hline
    \textbf{Legal concept} & \textbf{Generated question} \\
\hline
Effective legal remedies for data subjects & Are there established procedures within the organization for data subjects to seek legal remedies?\\
\hline
Legally binding and enforceable instrument & Does the instrument in question meet the criteria for being legally binding under relevant laws?\\
\hline
Standard data protection clauses & Has the organization implemented Standard Contractual Clauses (SCCs) for data transfers?\\
\hline
Standard data protection clauses & Are there mechanisms in place to review and update the standard data protection clauses in response to legal or operational changes?\\
  \end{tabular}
  \caption{Example questions generated by ChatGPT for the legal concepts taken from GDPR.}
  \label{tab:question-examples}
\end{table}

We note that the above process meets the interpretability and performance properties. The first because the outcome can be easily verified by humans and the third because the process is fully automatic and unsupervised. We refer to the accuracy property at the end of Sec. \ref{sec:experiments}, taking into account the actual accuracy rate of our model.

\subsection{Sentence retrieval}

The main element of the procedure is the retrieval of sentences within judgments relevant to the generated questions. For the experiment discussed in Sec. \ref{sec:experiments}, we have implemented only a cross-encoder, which is typically used for re-scoring the results on a subset of documents. The cross-encoder is run for each combination of generated question and 
passage from the decision corpus. 

Our goal for the cross-encoder is to score the likelihood that a certain passage contains information relating to the question.
For this purpose, we are using a binary classification model by fine-tuning AlBERT \cite{lan2019albert} model (from the BERT family
\cite{devlin2018bert} of encoder-only transformer models) with the SQuAD 2.0 dataset \cite{rajpurkar2018know}.

We had to adapt the dataset for the task at hand, as in SQuAD most of the questions are answerable, while in our task, the vast majority of answers are irrelevant. To resolve this problem we have pre-processed SQuAD 2.0 by splitting each answer into individual sentences. This is followed by mapping the sentence answering the question as a positive example, while all the other sentences \textbf{from the same passage} are treated as the negative examples for the same question.
We call this approach an indirect contrastive learning, since the negative examples will include information 
which is highly relevant for the question, but they do not include the actual answer. As a result we obtain a dataset that includes a high number of negative examples, making it more representative of our problem.

Since the model outputs a probability score for the question -- sentence pair, it is possible to sort the sentences according
to the decreasing probability of the sentence containing the answer to the question. But since this is a binary classification
model, by default pairs with probability below 0.5 will be judged as not containing the answer. This threshold can be increased
to obtain a higher accuracy of the results, as can be seen in Sec. \ref{sec:experiments}. 

Once again, the model satisfies two out of the three properties. The model is interpretable, since the answers are identical and contain references to the original phrases in judgements. It is also efficient as it is fully automatic and relatively simple (matching all questions with all sentences). The performance might be further improved by using a dense retriever \cite{karpukhin2020dense}
and approximate maximum product search with the help of e.g. Faiss \cite{johnson2019billion}.

\subsection{Passage classification}

For the last stage of the processing, we extend the sentences into contexts and classify them as positive or negative examples. 
Extending the questions is done by taking one preceding and one following sentence. It should be noted that finding more sophisticated approaches is left as a future work.
To classify the extended 
context into one of the two classes we use an instruction following model, in that case Flan-T5 \cite{chung2022scaling}. 
This is a T5-based model \cite{raffel2020exploring}, fine-tuned to follow instructions on a large group of various
English datasets. The prompt sent to the model is given in Figure \ref{fig:flan-instruction}.

\begin{figure}[htbp]
  \texttt{Given the context: '<context>' answer the following question by reasoning step-by-step: <question>} 
  \caption{The prompt used to ask Flan-T5 for classification of the examples.}
  \label{fig:flan-instruction}
\end{figure}

The result of the prompt is an explanation of why the model believes that the answer is either positive or negative.
It allows to group the examples into positive and negative ones by checking if the answer contains `yes'. Once again, this makes the model
interpretable on the human level, since the user is able to assess if the justification is sensible and eliminate inaccurate results. 
According to \cite{chia2023instructeval} Flan-T5 is a state-of-the-art instruction-following model among the models of similar size.
Still, the accuracy of the model could be improved if a dedicated dataset, with binary QA pairs would be created. Creation of such dataset is left for a future research.
