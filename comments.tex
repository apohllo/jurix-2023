The domain of the paper is not clear:
- Are we the only example based approach to QAs? In the legal domain? (A new QA system paper)
- Are we presenting a new way to create them only? (A new QA creation paper)

We need to get a better understanding of the sota. The latest paper on LQAS: A survey on legal question–answering systems by Jorge Martinez-Gil

He mentions 8 sureys in the general domain which needs to be processed:

[2] O. Kolomiyets, M. Moens, A survey on question answering technology
from an information retrieval perspective, Inform. Sci. 181 (24) (2011)
5412–5434, http://dx.doi.org/10.1016/j.ins.2011.07.047.
[3] M. Wang, A survey of answer extraction techniques in factoid question
answering, Comput. Linguist. 1 (1) (2006) 1–14.
[4] K. Höffner, S. Walter, E. Marx, R. Usbeck, J. Lehmann, A.N. Ngomo, Survey
on challenges of question answering in the semantic web, Semantic Web
8 (6) (2017) 895–920, http://dx.doi.org/10.3233/SW-160247.
[5] D. Diefenbach, V. López, K.D. Singh, P. Maret, Core techniques of question
answering systems over knowledge bases: a survey, Knowl. Inf. Syst. 55
(3) (2018) 529–569, http://dx.doi.org/10.1007/s10115-017-1100-y.
[6] W. Franco, C.V.S. Avila, A. Oliveira, G. Maia, A. Brayner, V.M.P. Vidal,
F. Carvalho, V.M. Pequeno, Ontology-based question answering systems
over knowledge bases: A survey, in: J. Filipe, M. Smialek, A. Brodsky, S.
Hammoudi (Eds.), Proceedings of the 22nd International Conference on
Enterprise Information Systems, ICEIS 2020, Prague, Czech Republic, May
5-7, 2020, Volume 1, SCITEPRESS, 2020, pp. 532–539, http://dx.doi.org/
10.5220/0009392205320539.
J. Martinez-Gil Computer Science Review 48 (2023) 100552
[7] E. Dimitrakis, K. Sgontzos, Y. Tzitzikas, A survey on question answering
systems over linked data and documents, J. Intell. Inf. Syst. 55 (2) (2020)
233–259, http://dx.doi.org/10.1007/s10844-019-00584-7.
[8] J.W.F. da Silva, A.D.P. Venceslau, J.E. Sales, J.G.R. Maia, V.C.M. Pinheiro,
V.M.P. Vidal, A short survey on end-to-end simple question answering
systems, Artif. Intell. Rev. 53 (7) (2020) 5429–5453, http://dx.doi.org/10.
1007/s10462-020-09826-5.
[9] A. Rogers, M. Gardner, I. Augenstein, QA dataset explosion: A taxonomy
of NLP resources for question answering and reading comprehension,
ACM Comput. Surv. 55 (10) (2023) 197:1–197:45, http://dx.doi.org/10.
1145/3560260

The main goal of tools according to the COLIEE competition is: Given a decision, automatically extract from cases a sentence which entails it

According to the paper, the main three tasks of LQAS are:
- Query understanding
- Source retrieval
- Answer extraction
But that most systems follow the tasks differently.

We have a different approach. We start with a concept in a certain context. For example Contractual Clauses in GDPR data transfers. We obtain yes/no questions regarding the concept in different contexts. We then look for sentences in cases which either positively demonstrate the question or negatively.

---
I wonder if we should not focus on our approach as starting with a concept and context and generating a question and answer at the same time. This seems original and can overcome the problem of matching an answer to a question.
---

If we do, we can say that we overcome some challenges of current lqas, such as the need to understand the meaning of a question, in order to match an answer. We just match examples.

Problem from survey: The major problem of
this model is that questions in human language express a well-
defined information requirement on the one hand. However, they
also convey more information than an essential list of relevant
keywords since they represent syntactic and semantic links be-
tween those keywords. Therefore, solutions work well only to a
limited extent, i.e., when the questions are simple.

The goal of multiple choice answers is: the goal is to learn a scoring function S(F , z) with a
normalization parameter z (whereby z or the normalization factor
is usually used so that the values associated with each answer are
in the same numerical range) such that the score of the correct
choice is higher than the score of the other hypotheses and their
corresponding probabilities.

is it the goal in our approach too?

Table 4 shows 6 different approaches. Are they relevant to our goal? Seems we mainly use co-occurrence? See p.5 for sota in this regards. Where do we stand?

Section 2.4 seems highly relevant. Do we use this approach? Do we overcome its weaknesses?

This seems closest to us and doesnt appear in the sota we have so far: If we restrict ourselves to LQA, one of the most popular ini-
tiatives is ResPubliQA. This evaluation task was proposed at the
Cross-Language Evaluation Forum (CLEF) [32,33] a consists of
given a pool of independent questions in natural language about
European legislation, proposed systems should return a passage
(but not an exact response) that answers each question. This ini-
tiative was a remarkable success. In fact, thanks to the ResPubliQA
competition, this family of methods has been the subject of
intensive research during the last decade. (See table 5!)

How is 2.6 relevant?

What we do is 2.7 but i cannot see the mentioned works in our sota.

=======

We claim that example based QA systems (or QE systems?) have adventages over QA systems in the ability to generate the systems, accuracy, volume and also usability. They might be more convenient for a certain type of client. We need to support all such arguments.

Can we state that that our approach is fully explainable according to the definition? (get all three: accuracy, interpretability, and performance)

=======


The idea is that the most acurate way to answer legal questions is by showing examples and letting the reader make a decision.

Why QA systems are important in general and in particular for the legal domain. <TODO>

We focus on legal QA systems creation. The sota for creation of QA systems in general and legal ones in particular is <TODO>. In addition, we focus on not answering questions but on giving positive and negative examples. What is the SotA in this direction? Is it similar to some systems in the health domain, as there are some similarities in this sense between health and legal?

What are the challenges? <TODO>
- Accuracy
- effort to build
- relevance to actual users

Our approach is to create fully automatically legal QA datasets of high accuracy.<TODO>. For the current paper, we generated the questions automatically.

Our goal is to make available to the public a website of QA regarding different legal problems, where the questions themselves will be rated by the users (according to number of visits, etc.) and will be easily searchable. Such a website will also enable us to ask the users to input common legal problems, which will form the basis of our dataset.

In this paper we introduce the approach and demonstrate on a small dataset of court case decisions. 

TASKS:
- what are the different types of QA systems creation and in particular legal ones?

Our approach is not to try a reading comprehension on a text and see what part of it answers a certain open question. But, to ask binary questions and choose texts which demonstrates a situation answered by the question in a positive or a negative way. This approach simulates the way a lawyer would approach a legal abstract problem by finding positive and negative examples and trying to apply them to a specific case at hand. In our approach, a question will associated with several examples and the user would be able to estimate  
Types of QA systems:

- information retrieval
- reading comprehension (squad)

Advancements in QA in various domains such as <give examples> shows a great potential of automatically answering questions based on data. In the legal domain, this is less successful because <write reasons>.

In this paper we describe a an approach for generating a legal QA systems by first gathering the legal vocabulary from a legislation, marking the articles it is extracted from, then using chatGPT prompt engineering to create the questions and then using an LLM fine-tuned on court case decisions to find the most relevant groups of sentences in court case decisions which answer the generated questions

We focus on binary questions which can be positively or negatively demonstrated by at most one paragraph in court cases. 

These questions differ from those in existing legal QA datasets as they are not completely answered but gather examples.

The logic behind it is to avoid answering legal questions, which is beyond the capabilities of a machine, but to help both legal professionals and the general public to understand how courts interpret the questions by showing positive and negative examples.
