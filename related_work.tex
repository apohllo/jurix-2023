


\section{Related work}

The discussion of related work in question answering was divided into three parts. The first part concerns the most widely
used QA datasets in general. The second group of datasets are those related specifically to the law domain, while
the last part discusses the QA datasets available for Polish.



\subsection{QA in English}

Regarding question answering in English there is a growing number of domain-independent datasets: famous SQuAD
\cite{rajpurkar2016squad}, WikiQA \cite{yang2015wikiqa}, CNN/DailyMail \cite{hermann2015teaching}, MS MARCO
\cite{bajaj2016ms}, TriviaQA \cite{joshi2017triviaqa}, Natural Questions (NQ) \cite{kwiatkowski2019natural}, to name just
the most popular of them.

SQuAD, WikiQA and NQ all use Wikipedia as a reference knowledge base for the answers. These dataset
differ with respect to the size and the way they were constructed. The questions for SQuAD were provided by humans \cite{rajpurkar2016squad}, but in an artificial setting, when the annotator was given a task to invent a number of questions related to an excerpt of Wikipedia article, provided by the dataset creators. The assumption for the first version of the dataset was that the answer for
the question have to be present in the snippet. Then for each such question a number of annotators had to select a consecutive sequence of tokens, which constitute the answer for that question. The number of excerpts was more than 500, while the dataset of question-answer pairs (treating multiple answers for the same question as one answer) included more than 100 thousand
entries. The second version of the dataset (SQuAD 2.0 \cite{rajpurkar2018know}) includes additional 50 thousand questions that didn't have an answer in the provided text. Even though the dataset is very large (more than 100 thousand entries), the fact that there are only 500 excerpts and the fact that the annotators had to invent these questions are the primary factors that limit its usefulness. [TODO A: the last paragraph is not clear. Why is it not useful? How are the arguments relevant to the conclusion?]

WikiQA \cite{yang2015wikiqa}, which preceded SQuAD, used Bing queries as the set of questions and Wikipedia pages as the source for the answers. It includes more than 3 thousand questions which are linked to Wikipedia pages, potentially including the answer. The summary section of the pages were sentence-split, yielding almost 30 thousand sentences. Among them 1473 sentences were selected as the answers to the questions. Comparing to SQuAD, WikiQA has the advantage of having only natural questions, but it is orders of magnitude smaller. Moreover, providing full sentences as answers makes the problem more similar to selective (also known as \textit{passage retrieval} problem), rather than extractive QA task.

Natural Questions \cite{kwiatkowski2019natural} are similar to WikiQA, since they include as questions queries from Google search engine, but they form a much larger dataset. It includes more than 300 thousand training examples (annotated by one person), almost 8 thousand pairs annotated by 5 people, as a development set and  similar number of testing examples annotated by 5 people. Another difference is the answer format: NQ contains long and short answers. Long answers are typically a paragraph of Wikipedia text, while the short answers usually refer to a single entity, thus they
are very similar to the answers in SQuAD. As a result NQ may be used to train and validate performance on the algorithms both in the selective and extractive QA schemes. [TODO A: various numbers and passages look a bit dubious. 300k annotated by 1 person?]

CNN/Daily mail dataset \cite{hermann2015teaching} differs with the previous datasets by using articles from CNN and Daily Mail, rather than Wikipedia, making the textual material more challenging, since the encyclopedic style of Wikipedia allows to answer many questions just by citing an excerpt. In the case of newspaper articles a good answer requires not only the background knowledge that may be absent from the article, but also the acquaintance with more
sophisticated style and vocabulary, characteristic for the content that need to be informative, entertaining and succinct at the same time. Yet in the case of that dataset, the QA task was formulated as a cloze task, i.e. the system has to provide the answer as a word that fills a blank in a declarative sentence, e.g. \textit{Producer X will not press charges against Jeremy Clarkson, his lawyer says}.

The authors of the dataset made the task much more challenging by replacing all named entities in the text with abstract identifiers. This prevents the model from utilizing linguistic bias present in language models, since these identifiers are absent from any texts, used to train them. The dataset
contains 93 thousand articles from CNN and 220 thousand articles from Daily Mail and there are 1 million clozes, generated from the summaries of these articles.

The drawbacks of this dataset in the context of question answering are partly artificial texts (due to replaced entities) as well as the style of the task, which allows to evaluate the models for their linguistic ,,skills'', but since the sentences are declarative, they are not real questions. A smart solution which, has nothing to do with language knowledge, may limit the set of possible answer to the artificial entities, which is much smaller than the total number of words in the text. The set of  possible answer spans is obviously much larger.

An interesting dataset, similar in spirit to NQ is MS MARCO \cite{bajaj2016ms}. It contains more than 1 million questions taken from the Bing queries, almost 9 million passages (not limited to Wikipedia) potentially containing the information necessary to provide the answer and approx. 180 thousand answers provided as full sentences. It differs from NQ by providing abstractive questions and by not limiting itself to the encyclopedic content of Wikipedia.

The last dataset related to open-domain QA in English that we discuss in this section is TriviaQA
\cite{joshi2017triviaqa}. Its unique feature is the fact that the questions were gathered from trivia websites. The questions were supplemented with webpages and Wikipedia articles, containing the answer to the questions. The authors claim that the questions are more challenging for the question answering system, compared to datasets such as SQuAD, since they usually require reasoning over more than one sentence. The dataset contains 95 thousand question-answer pairs (with 40 thousand unique answers) and 650 thousand question-answer-document triples. At the time of release the best performing
models reached only 40\% exact match score, which was far below human performance (80\%). The most recent models such as SpanBERT \cite{joshi2020spanbert} reaches 86\% (F1 score) on this dataset.

It is apparent that the size, the variability and the difficulty of the questions characteristic for these dataset is much diversified. But they also have their limitations: all these dataset are available only for English and none of them concentrates on the law domain.

\subsection{QA datasets related to law}

Regarding QA in the domain of law, the number of dataset is much smaller and their variability is much lower. We have identified the following relevant datasets: Competition on Legal Information Extraction/Entailment (COLIEE) including its most recent edition (2021)\footnote{The results for the 2022 edition of the competition are not available yet.}, where tasks 3, 4 and 5 concentrate on Legal Question Answering \cite{rabelo2021summary}, JEC-QA a Chinese legal QA dataset \cite{zhong2019jecqa}, another Chinese dataset called AILA \cite{weiyi2020aila} and a dataset for evaluating legal question answering on private international law \cite{sovrano2021dataset}. There is also a very large dataset constructed by the Westlaw company (it contains approx.  200 thousand QA pairs) \cite{mcelvain2019westsearch}, but it is not publicly available, so it is excluded from this overview.

COLIEE is a competition aimed at fostering the research on algorithms for legal information retrieval, extraction and understanding. The first two tasks during the latest edition of COLIEE (2021) were aimed at case law, while the remaining 3 tasks were all related to question answering \cite{rabelo2021summary}. The aim of Task 3 -- statute law information retrieval, was searching for a subset of Japanese Civil Code Articles, relevant for answering a legal bar exam question.
The aim of Task 4 -- statute law textual entailment, was to determine if there exist an entailment relationship between a given problem sentence and the article sentence, while the aim of Task 5, was determining the binary answer to the problem sentence, based on any external information, with the articles from Task 3 being the authoritative sources.  Task 3 includes 806 questions in the training set, 81 questions in the tests set and 768 articles in the database. The questions and the articles are provided both in Japanese and in English. The majority of the questions (80\%) has only one relevant article in the whole dataset. The dataset for tasks 4 and 5 have the same questions, but the answers are adjusted to the problem. In both cases the answers provided in the dataset are binary
(there is/isn't entailment between the sentences, the answer to the question is yes/no).

JEC-QA \cite{zhong2019jecqa} is a dataset collecting questions from National Judicial Examination of China (NJEC) and websites for examination. Passing the exam is necessary to become a legal professional in China and only 10\% of them are successful at this task. The dataset contains more than 26 thousand questions with multiple-choice answers (each containing 4 options). These questions were collected from the official exams (2700 questions) and from websites containing exercises
preparing for the exam. The dataset also includes the database necessary to answer these questions, which is based on National Unified Legal Professional Qualification Examination Counseling Book and Chinese legal provision (3382 in total). The questions are annotated with additional metadata including the type of the question as well as the reasoning abilities required by the question.
The dataset follows OpenQA setting \cite{chen2017reading}, i.e. answering a questions requires sophisticated retrieval and reasoning capabilities, since the correct answer might require reading more than one source document, multi-hop reasoning and numerical analysis. The hardness of the problem results in low performance of the general methods for question answering, reaching only 25-29\% correctness for the best models.

The AILA dataset \cite{weiyi2020aila} was constructed by scraping Chinese legal forum, where legal questions are answered by legal professionals. The dataset includes almost 140 thousand QA pairs. According to the article, it seems that the dataset follows selective QA, since the authors evaluated their QA module by measuring Mean Reciprocal Rank and Mean Average Precision -- measures typically used for information retrieval systems assessment. But since the description is very short and gives only one example translated to English, it might be possible that the dataset also allows to train an
extractive QA system.

The last dataset aimed at question answering in the legal domain is a Dataset for Evaluating Legal Question Answering on Private International Law (PIL) \cite{sovrano2021dataset} (henceforth called PIL dataset). That work is expansion of \cite{sovrano2020legal} -- both of them were based on the following regulations (in English): 
\begin{enumerate}
  \item Rome I Regulation EC 593/2008;
  \item  Rome II Regulation EC 864/2007;
  \item Brussels I bis Regulation EU 1215/2012.
\end{enumerate}
The regulations are defined in the context of international law, making their application a particularly challenging task. Yet the questions -- their interpretation in particular -- were posed as occupying self-contained environment, i.e. abstracting from any other regulations that may interfere with them. The questions were thought up by legal experts and answered by different legal experts as indications of Articles, Recitals or Commission Statements from the indicated
regulations. As such this dataset follows the selective QA paradigm. The dataset is of very high quality, but it includes only 17 questions altogether.