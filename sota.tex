Following the analysis given in \cite{martinez2023survey} and the recent developments in neural QAS, 
it is safe to say that Legal QAS fall into one of three categories:
\begin{enumerate}
    \item knowledge-based systems (using ontologies and linked data), e.g. \cite{fawei2018methodology, filtz2021linked},
    \item retrieve, re-rank and interpret systems, e.g. \cite{zhong2020building},
    \item large langue model based systems, e.g. \cite{chung2022scaling}.
\end{enumerate}

An example of the first approach is given in \cite{fawei2018methodology}. The authors manually build an ontology
for criminal law and supplement it with rules. Answering questions is possible thanks
to an inference engine, which draws conclusions based on the interpretations of the legal concepts and 
application of the rules. The key benefits of that approach are explainability and accuracy, but this approach
suffers from scalability issues, since the ontology and logical rules are manually created. Moreover 
the inference engines are slow, which negatively influences the performance of such systems \cite{martinez2023survey}.

The example of the second approach is given in \cite{zhong2020building}, where the authors describe a system
helping engineers in China to find legislation regulating the design and development of buildings.
The system is based on a combination of sparse retrieval and deep learning. When the user asks a question,
the system searches for relevant law in a database containing the relevant legislation using TF-IDF term
weighting scheme \cite{manning1999foundations}. Then for top-n returned passages (the system does not have a re-ranking
module) a Chinese BERT-like model is applied. The model is fine-tuned on a collection of question-passage-answer triplets, to 
perform extractive question answering (the answer has to be present literally in the passage). The system is interpretable
on the human level (the returned passages and the answer location might be inspected manually), but has a limited scalability,
In addition, the terms in the question must match the terms in the legislation directly, due to the application of 
a sparse retrieval algorithm. To overcome that problem a thesaurus would have to be built or a dense retriever such as DPR \cite{karpukhin2020dense} would have
to be applied.

ChatGPT \cite{chung2022scaling} is an example of a system of the third type. Even though these system achieve 
state-of-the-art results in legal QA\footnote{\cite{openai2023gpt4} claims that GPT 4.0 scores among 10\% of the top test takers 
for the Uniform Bar Exam, but the paper is a pre-print and ,,the authors'' do not provide the most important technical details
of the system}, they have one very important limitation -- they are not explainable and they have a 
tendency to hallucinate, i.e. they can make up facts. In the USA, two lawyers were recently fined for citing non-existent cases
reported by ChatGPT \cite{ap2023lawyers}.

Our system is most similar to the second approach, but it differs in the following ways:
\begin{enumerate}
    \item the system does not give an answer, but finds positive and negative examples,
    \item the system takes a legal concept as the input,
    \item the retrieval is based solely on a deep neural network,
    \item the queries (questions) are generated automatically by a large language model.
\end{enumerate}

We claim that the system presented in the next two sections achieves a similar accuracy level to the ones mentioned above while being more explainable and scalable.

%Overview sota with regards to the three paramters:
%
%Explainability is obtained by keeping the original text and keeping references to original judgments
%Accuracy is obtained by not answering a question but framing the answer using real positive and negative examples
%Scalabilty is obtained by the use of unsupervised learning.

%\cite{zhong2020building} is an example of a system that utilizes deep learning for factoid
%question answering, following the retrieve-extract paradigm. The authors developed a system
%that is aimed at answering questions related to the Chinese building regulations. 
%The corpus contains the documents from the Chinese law related to buildings. These documents
%are split into passages following the structure of provisions -- each provision is treated 
%as a tree and for each non-leaf node a passage containing the content of parent nodes and all 
%descendant nodes is created.
%
%The retrieval model is based on a sparse representation of the passages -- the authors use TF-IDF 
%\cite{manning1999foundations} vector model for retrieval. 
%The training size of the dataset contains 2500 triplets, while the validation and the testing
%part contain 500 triplets each.
%
%The authors report very high accuracy of the answers (95\% exact match score for factoid questions 
%and 90\% for definitional questions), but this only comprises the answer extraction module. 
%The authors do not provide a similar scores for the complete system, but only report the user 
%evaluation in terms of easy of use, quality of answers and time spent by the user. The overall performance 
%of the system judged by the users is better compared to Baidu search engine and regulation document 
%database (however the last system is judged better in terms of accuracy of the answers).